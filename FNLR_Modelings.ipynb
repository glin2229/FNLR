{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Modeling Setup"
      ],
      "metadata": {
        "id": "_YnXY5v1iOKI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "EVd_c0nIbiPY"
      },
      "outputs": [],
      "source": [
        "# First, please setup the essential hyper-param for modeling and select the dataset you would like to model; then run each code block sequentially\n",
        "# Note: this code requires a GPU to execute (ex. T4 or L4, with high-mem enabled)\n",
        "\n",
        "# set hyper-params here:\n",
        "model_type = 'base_sub_no_mask' # valid inputs: base_no_mask (adapt a DTree), base_sub_no_mask (adapt a DTree with several sub-trees)\n",
        "tree_depth = 10 # depth of the symbolic DTree to adapt\n",
        "masking = False # whether to perform dropout during logical regularizer computation, default to False\n",
        "bayes_opt = True # perform bayesian optimization for hyper-param\n",
        "\n",
        "# select dataset to model by commenting out others:\n",
        "# dataset = 'Higgs' # valid inputs: Higgs, Census, Credit, Insurance, Cover\n",
        "# non_ordinal_cat = False # set True if there exists non-ordinal Categorical feature(s) in above dataset\n",
        "# cat_as_num = False # set True to treat the non-ordinal Categorical feature(s) in above dataset as numericals; default to False\n",
        "\n",
        "# dataset = 'Census'\n",
        "# non_ordinal_cat = True\n",
        "# cat_as_num = False\n",
        "\n",
        "# dataset = 'Credit'\n",
        "# non_ordinal_cat = False\n",
        "# cat_as_num = False\n",
        "\n",
        "# dataset = 'Insurance'\n",
        "# non_ordinal_cat = True\n",
        "# cat_as_num = False\n",
        "\n",
        "dataset = 'Cover'\n",
        "non_ordinal_cat = True\n",
        "cat_as_num = False\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nv1C11Jmb5_1",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "!pip install category_encoders\n",
        "!pip install optuna"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "3QUgojl7gaK_"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "import sklearn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import metrics\n",
        "from sklearn import preprocessing\n",
        "from category_encoders import TargetEncoder\n",
        "from sklearn.tree import export_text\n",
        "import optuna\n",
        "import pickle\n",
        "from sklearn.preprocessing import QuantileTransformer\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "\n",
        "import copy\n",
        "from itertools import chain\n",
        "import random\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn import tree\n",
        "from sklearn.tree import _tree\n",
        "\n",
        "torch.set_printoptions(sci_mode = False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "7-ny2RI-h4gW"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "le9uMZR1Plux"
      },
      "outputs": [],
      "source": [
        "class EarlyStopper:\n",
        "    def __init__(self, patience = 1, min_delta = 0):\n",
        "        self.patience = patience\n",
        "        self.min_delta = min_delta\n",
        "        self.counter = 0\n",
        "        self.min_validation_loss = np.inf\n",
        "\n",
        "    def early_stop(self, validation_loss):\n",
        "        if validation_loss < self.min_validation_loss:\n",
        "            self.min_validation_loss = validation_loss\n",
        "            self.counter = 0\n",
        "        elif validation_loss >= (self.min_validation_loss + self.min_delta):\n",
        "            self.counter += 1\n",
        "            if self.counter >= self.patience:\n",
        "                return True\n",
        "        return False\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "_Whkv9P_PmBg"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "WhOdaH11i9Pi"
      },
      "outputs": [],
      "source": [
        "def get_rules(tree, feature_names, class_names):\n",
        "    tree_ = tree.tree_\n",
        "    feature_name = [\n",
        "        feature_names[i] if i != _tree.TREE_UNDEFINED else \"undefined!\"\n",
        "        for i in tree_.feature\n",
        "    ]\n",
        "\n",
        "    paths = []\n",
        "    path = []\n",
        "\n",
        "    def recurse(node, path, paths):\n",
        "\n",
        "        if tree_.feature[node] != _tree.TREE_UNDEFINED:\n",
        "            name = feature_name[node]\n",
        "            threshold = tree_.threshold[node]\n",
        "            p1, p2 = list(path), list(path)\n",
        "            p1 += [f\"{name} <= {np.round(threshold, 3)}\"]\n",
        "            recurse(tree_.children_left[node], p1, paths)\n",
        "            p2 += [f\"{name} > {np.round(threshold, 3)}\"]\n",
        "            recurse(tree_.children_right[node], p2, paths)\n",
        "        else:\n",
        "            paths += [path]\n",
        "\n",
        "    recurse(0, path, paths)\n",
        "\n",
        "    return paths\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "jpqriiXAaDGb"
      },
      "outputs": [],
      "source": [
        "# Neural Symbolic Decision Tree (denoted as NSDT) is the model developed according to our proposed FNLR paradigm:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "z7KUEiUfaDUF"
      },
      "outputs": [],
      "source": [
        "class MLP(nn.Module):\n",
        "\n",
        "    def __init__(self, MLP_type, embedding_dimension, hidden_init, hidden_nodes_layer1, hidden_nodes_layer2, activation):\n",
        "        super(MLP, self).__init__()\n",
        "        self.MLP_type = MLP_type\n",
        "\n",
        "        if (self.MLP_type == 'module'):\n",
        "\n",
        "            if (activation == 'leakyrelu'):\n",
        "                self.act = nn.LeakyReLU()\n",
        "            if (activation == 'relu'):\n",
        "                self.act = nn.ReLU()\n",
        "            if (activation == 'elu'):\n",
        "                self.act = nn.ELU()\n",
        "            if (activation == 'gelu'):\n",
        "                self.act = nn.GELU()\n",
        "            if (activation == 'tanh'):\n",
        "                self.act = nn.Tanh()\n",
        "\n",
        "            self.sig = nn.Sigmoid()\n",
        "\n",
        "            self.l1 = nn.Linear(embedding_dimension * 2, hidden_nodes_layer1)\n",
        "            self.l2 = nn.Linear(hidden_nodes_layer1, hidden_nodes_layer2)\n",
        "            self.output = nn.Linear(hidden_nodes_layer2, 1)\n",
        "\n",
        "            if (hidden_init == 'xavier'):\n",
        "                nn.init.xavier_uniform_(self.l1.weight)\n",
        "                nn.init.xavier_uniform_(self.l2.weight)\n",
        "                nn.init.xavier_uniform_(self.output.weight)\n",
        "            if (hidden_init == 'he'):\n",
        "                nn.init.kaiming_uniform_(self.l1.weight)\n",
        "                nn.init.kaiming_uniform_(self.l2.weight)\n",
        "                nn.init.kaiming_uniform_(self.output.weight)\n",
        "\n",
        "        else:\n",
        "            self.output = nn.Linear(len(rules), 1) # rules is global still\n",
        "            nn.init.xavier_uniform_(self.output.weight)\n",
        "\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        if (self.MLP_type == 'module'):\n",
        "            return self.sig( self.output( self.act( self.l2( self.act( self.l1(inputs) ) ) ) ) )\n",
        "        else:\n",
        "            return self.output(inputs)\n",
        "\n",
        "\n",
        "\n",
        "class NSDT(nn.Module):\n",
        "\n",
        "    def __init__(self, masking, total_levels, embedding_dimension, hidden_init, hidden_nodes_layer1, hidden_nodes_layer2, activation):\n",
        "        super(NSDT, self).__init__()\n",
        "        self.total_levels = total_levels\n",
        "        self.embedding_dimension = embedding_dimension\n",
        "        self.hidden_init = hidden_init\n",
        "        self.hidden_nodes_layer1 = hidden_nodes_layer1\n",
        "        self.hidden_nodes_layer2 = hidden_nodes_layer2\n",
        "        self.activation = activation\n",
        "        self.NSDT_masking = masking\n",
        "\n",
        "        self.ge_module = MLP('module', self.embedding_dimension, self.hidden_init, self.hidden_nodes_layer1, self.hidden_nodes_layer2, self.activation)\n",
        "        self.le_module = MLP('module', self.embedding_dimension, self.hidden_init, self.hidden_nodes_layer1, self.hidden_nodes_layer2, self.activation)\n",
        "        self.be_module = MLP('module', self.embedding_dimension, self.hidden_init, self.hidden_nodes_layer1, self.hidden_nodes_layer2, self.activation)\n",
        "        self.target_projection = MLP('target', self.embedding_dimension, self.hidden_init, self.hidden_nodes_layer1, self.hidden_nodes_layer2, self.activation) # also passed in, just not used\n",
        "\n",
        "        self.num_feature_embeddings = nn.Embedding(self.total_levels, self.embedding_dimension)\n",
        "        self.context_value_embeddings = nn.Embedding(be_node_counter, self.embedding_dimension) # be_node_counter global variable still\n",
        "\n",
        "\n",
        "    def forward(self, batch_le_feature_index_list, batch_le_context_value_index_list,\n",
        "                batch_ge_feature_index_list, batch_ge_context_value_index_list,\n",
        "                batch_be_feature_index_list, batch_be_context_value_index_list, batch_size):\n",
        "\n",
        "        # compute le nodes\n",
        "        feature_batch_embed = self.num_feature_embeddings(batch_le_feature_index_list).to('cuda') # 440 x 128 x 200\n",
        "\n",
        "        lookup_tensor = torch.tensor(batch_le_context_value_index_list, dtype = torch.long).to('cuda')\n",
        "        context_value_embed = self.context_value_embeddings(lookup_tensor).to('cuda') # 440 x 128 x 200\n",
        "\n",
        "        le_module_input = torch.cat( [feature_batch_embed, context_value_embed], axis = 2 ).requires_grad_(True)\n",
        "\n",
        "        le_nodes_output = self.le_module.forward(le_module_input).requires_grad_(True) # 440 x 128 x 1\n",
        "        le_nodes_output = torch.squeeze(le_nodes_output).permute(1,0).requires_grad_(True)\n",
        "\n",
        "        # compute ge nodes\n",
        "        feature_batch_embed = self.num_feature_embeddings(batch_ge_feature_index_list).to('cuda') # 56320 x 200\n",
        "\n",
        "        lookup_tensor = torch.tensor(batch_ge_context_value_index_list, dtype = torch.long).to('cuda')\n",
        "        context_value_embed = self.context_value_embeddings(lookup_tensor).to('cuda')\n",
        "\n",
        "        ge_module_input = torch.cat( [feature_batch_embed, context_value_embed], axis = 2 ).requires_grad_(True)\n",
        "\n",
        "        ge_nodes_output = self.ge_module.forward(ge_module_input).requires_grad_(True) # 425 x 128 x 1\n",
        "        ge_nodes_output = torch.squeeze(ge_nodes_output).permute(1,0).requires_grad_(True)\n",
        "\n",
        "        # copmute be nodes\n",
        "        feature_batch_embed = self.num_feature_embeddings(batch_be_feature_index_list).to('cuda') # 56320 x 200\n",
        "\n",
        "        lookup_tensor = torch.tensor(batch_be_context_value_index_list, dtype = torch.long).to('cuda')\n",
        "        context_value_embed = self.context_value_embeddings(lookup_tensor).to('cuda')\n",
        "\n",
        "        be_module_input = torch.cat( [feature_batch_embed, context_value_embed], axis = 2 ).requires_grad_(True)\n",
        "\n",
        "        be_nodes_output = self.be_module.forward(be_module_input).requires_grad_(True) # 425 x 128 x 1\n",
        "        be_nodes_output = torch.squeeze(be_nodes_output).permute(1,0).requires_grad_(True)\n",
        "\n",
        "\n",
        "        # cat all-1 vector by the end of le & ge node outputs;\n",
        "        padding_tensor = torch.ones( (batch_size, 1) ).to('cuda')\n",
        "        nodes_output = torch.cat( [le_nodes_output, ge_nodes_output, be_nodes_output, padding_tensor], axis = 1 ).requires_grad_(True) # 128 x 865\n",
        "\n",
        "        if (self.NSDT_masking == True):\n",
        "            nodes_dropout = torch.tensor( np.random.choice( [0, 1], size = (len(rules), tree_depth), p=[0.05, 0.95] ) ).to('cuda')\n",
        "            nodes_output = nodes_output[:, rule_look_up_indexes] * nodes_dropout\n",
        "            nodes_output = torch.where( nodes_output != 0, nodes_output, random.random() ).requires_grad_(True)\n",
        "\n",
        "            rule_outputs = torch.prod(nodes_output, dim = 2).requires_grad_(True)\n",
        "\n",
        "        else:\n",
        "            rule_outputs = torch.prod(nodes_output[:, rule_look_up_indexes], dim = 2).requires_grad_(True)\n",
        "\n",
        "        return self.target_projection(rule_outputs)\n",
        "\n",
        "\n",
        "\n",
        "class NSDT_num_feature_only(nn.Module):\n",
        "\n",
        "    def __init__(self, masking, total_levels, embedding_dimension, hidden_init, hidden_nodes_layer1, hidden_nodes_layer2, activation):\n",
        "        super(NSDT_num_feature_only, self).__init__()\n",
        "        self.total_levels = total_levels\n",
        "        self.embedding_dimension = embedding_dimension\n",
        "        self.hidden_init = hidden_init\n",
        "        self.hidden_nodes_layer1 = hidden_nodes_layer1\n",
        "        self.hidden_nodes_layer2 = hidden_nodes_layer2\n",
        "        self.activation = activation\n",
        "        self.NSDT_masking = masking\n",
        "\n",
        "        self.ge_module = MLP('module', self.embedding_dimension, self.hidden_init, self.hidden_nodes_layer1, self.hidden_nodes_layer2, self.activation)\n",
        "        self.le_module = MLP('module', self.embedding_dimension, self.hidden_init, self.hidden_nodes_layer1, self.hidden_nodes_layer2, self.activation)\n",
        "        self.be_module = MLP('module', self.embedding_dimension, self.hidden_init, self.hidden_nodes_layer1, self.hidden_nodes_layer2, self.activation)\n",
        "        self.target_projection = MLP('target', self.embedding_dimension, self.hidden_init, self.hidden_nodes_layer1, self.hidden_nodes_layer2, self.activation) # also passed in, just not used\n",
        "\n",
        "        self.num_feature_embeddings = nn.Embedding(self.total_levels, self.embedding_dimension)\n",
        "        self.context_value_embeddings = nn.Embedding(be_node_counter, self.embedding_dimension) # be_node_counter global variable still\n",
        "\n",
        "\n",
        "    def forward(self, batch_le_feature_index_list, batch_le_context_value_index_list,\n",
        "                batch_ge_feature_index_list, batch_ge_context_value_index_list, batch_size):\n",
        "\n",
        "        # compute le nodes\n",
        "        feature_batch_embed = self.num_feature_embeddings(batch_le_feature_index_list).to('cuda') # 440 x 128 x 200\n",
        "\n",
        "        lookup_tensor = torch.tensor(batch_le_context_value_index_list, dtype = torch.long).to('cuda')\n",
        "        context_value_embed = self.context_value_embeddings(lookup_tensor).to('cuda') # 440 x 128 x 200\n",
        "\n",
        "        le_module_input = torch.cat( [feature_batch_embed, context_value_embed], axis = 2 ).requires_grad_(True)\n",
        "\n",
        "        le_nodes_output = self.le_module.forward(le_module_input).requires_grad_(True) # 440 x 128 x 1\n",
        "        le_nodes_output = torch.squeeze(le_nodes_output).permute(1,0).requires_grad_(True)\n",
        "\n",
        "        # compute ge nodes\n",
        "        feature_batch_embed = self.num_feature_embeddings(batch_ge_feature_index_list).to('cuda') # 56320 x 200\n",
        "\n",
        "        lookup_tensor = torch.tensor(batch_ge_context_value_index_list, dtype = torch.long).to('cuda')\n",
        "        context_value_embed = self.context_value_embeddings(lookup_tensor).to('cuda')\n",
        "\n",
        "        ge_module_input = torch.cat( [feature_batch_embed, context_value_embed], axis = 2 ).requires_grad_(True)\n",
        "\n",
        "        ge_nodes_output = self.ge_module.forward(ge_module_input).requires_grad_(True) # 425 x 128 x 1\n",
        "        ge_nodes_output = torch.squeeze(ge_nodes_output).permute(1,0).requires_grad_(True)\n",
        "\n",
        "        # cat all-1 vector by the end of le & ge node outputs;\n",
        "        padding_tensor = torch.ones( (batch_size, 1) ).to('cuda')\n",
        "        nodes_output = torch.cat( [le_nodes_output, ge_nodes_output, padding_tensor], axis = 1 ).requires_grad_(True) # 128 x 865\n",
        "\n",
        "        if (self.NSDT_masking == True):\n",
        "            nodes_dropout = torch.tensor( np.random.choice( [0, 1], size = (len(rules), tree_depth), p=[0.3, 0.7] ) ).to('cuda')\n",
        "            nodes_output = nodes_output[:, rule_look_up_indexes] * nodes_dropout\n",
        "            nodes_output = torch.where( nodes_output != 0, nodes_output, random.random() ).requires_grad_(True)\n",
        "\n",
        "            rule_outputs = torch.prod(nodes_output, dim = 2).requires_grad_(True)\n",
        "\n",
        "        else:\n",
        "            rule_outputs = torch.prod(nodes_output[:, rule_look_up_indexes], dim = 2).requires_grad_(True)\n",
        "\n",
        "        return self.target_projection(rule_outputs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "O0b7invQjZo9"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "bCTPYfhBhJ7v"
      },
      "outputs": [],
      "source": [
        "def Train_NSDT_split_regs(masking, train_loader, val_loader, test_loader, early_stopper, new_train, cut_off_dict, feature_lookup_dict, cat_bins_total, total_numerical_nodes, total_levels, bins, embedding_dimension, n_iter, mlp_optimizer, hidden_init, hidden_nodes_layer1, hidden_nodes_layer2, activation, learning_rate, weight_decays, reg_frequency, le_reg_weight, ge_reg_weight, be_reg_weight):\n",
        "\n",
        "    be_weight = 1.0\n",
        "    if dataset == 'Higgs':\n",
        "        be_weight = 2.0\n",
        "\n",
        "    feature_indexes_le_nodes, feature_indexes_ge_nodes, feature_indexes_be_nodes, all_batch_le_context_value_index_list, all_batch_ge_context_value_index_list, all_batch_be_context_value_index_list, cat_features_Si_indexes, le_orderings_Si_indexes, ge_orderings_Si_indexes, le_ref_lookup_indexes, le_features_Si_indexes, ge_ref_lookup_indexes, ge_features_Si_indexes, le_current_feature_indexes, le_next_feature_indexes, le_expected_outputs_1, le_expected_outputs_2, ge_current_feature_indexes, ge_next_feature_indexes, ge_expected_outputs_1, ge_expected_outputs_2 = NSDT_modeling_preps(bins, train_loader, new_train, cut_off_dict, feature_lookup_dict, cat_bins_total, total_numerical_nodes, total_levels)\n",
        "\n",
        "    clf = NSDT(masking, total_levels, embedding_dimension, hidden_init, hidden_nodes_layer1, hidden_nodes_layer2, activation).to('cuda')\n",
        "\n",
        "    in_lookup_tensor = torch.tensor( list( range(total_numerical_nodes, be_node_counter) ), dtype = torch.long).to('cuda')\n",
        "    total_cat_nodes = be_node_counter - total_numerical_nodes\n",
        "\n",
        "    relu = torch.nn.ReLU()\n",
        "\n",
        "    pos_weight = torch.tensor( [training_reweight] ).to('cuda') # training_reweight is global still\n",
        "    bce_loss = nn.BCEWithLogitsLoss(pos_weight = pos_weight, reduction = 'mean')\n",
        "\n",
        "    if (mlp_optimizer == 'adam'):\n",
        "        optimizer = optim.Adam(clf.parameters(), lr = learning_rate, weight_decay = weight_decays)\n",
        "    else:\n",
        "        optimizer = optim.AdamW(clf.parameters(), lr = learning_rate, weight_decay = weight_decays)\n",
        "\n",
        "    print('Start Training:')\n",
        "    print()\n",
        "\n",
        "    accs = []\n",
        "    for epoch in range(n_iter):\n",
        "\n",
        "        total_loss = 0.0\n",
        "        batch_count = 0\n",
        "        reg_count = 0\n",
        "\n",
        "        for batch in train_loader:\n",
        "\n",
        "            subset = batch[:,:num_of_predictors] # X\n",
        "            batch_size = len(batch)\n",
        "\n",
        "            target = batch[:,num_of_predictors].to('cuda') # y\n",
        "            target = target.float()\n",
        "\n",
        "            batch_le_feature_index_list = subset[:, feature_indexes_le_nodes].permute(1,0).to('cuda')\n",
        "            batch_ge_feature_index_list = subset[:, feature_indexes_ge_nodes].permute(1,0).to('cuda')\n",
        "            batch_be_feature_index_list = subset[:, feature_indexes_be_nodes].permute(1,0).to('cuda')\n",
        "\n",
        "            batch_le_context_value_index_list = all_batch_le_context_value_index_list[batch_count]\n",
        "            batch_ge_context_value_index_list = all_batch_ge_context_value_index_list[batch_count]\n",
        "            batch_be_context_value_index_list = all_batch_be_context_value_index_list[batch_count]\n",
        "\n",
        "            preds = clf.forward(batch_le_feature_index_list, batch_le_context_value_index_list,\n",
        "                                batch_ge_feature_index_list, batch_ge_context_value_index_list,\n",
        "                                batch_be_feature_index_list, batch_be_context_value_index_list,\n",
        "                                batch_size)\n",
        "            preds = torch.squeeze(preds)\n",
        "\n",
        "\n",
        "            # add regularizers losses once every N batch:\n",
        "            total_in_loss = torch.zeros(1).to('cuda')\n",
        "            total_be_loss = torch.zeros(1).to('cuda')\n",
        "\n",
        "            total_le_loss = torch.zeros(1).to('cuda')\n",
        "            total_ge_loss = torch.zeros(1).to('cuda')\n",
        "\n",
        "            le_total_ref_loss = torch.zeros(1).to('cuda')\n",
        "            le_total_asym_loss = torch.zeros(1).to('cuda')\n",
        "            le_total_trans_loss = torch.zeros(1).to('cuda')\n",
        "            le_total_ordering_loss = torch.zeros(1).to('cuda')\n",
        "            le_total_cutoff_loss = torch.zeros(1).to('cuda')\n",
        "            le_total_comp_loss = torch.zeros(1).to('cuda')\n",
        "\n",
        "            ge_total_ref_loss = torch.zeros(1).to('cuda')\n",
        "            ge_total_asym_loss = torch.zeros(1).to('cuda')\n",
        "            ge_total_trans_loss = torch.zeros(1).to('cuda')\n",
        "            ge_total_ordering_loss = torch.zeros(1).to('cuda')\n",
        "            ge_total_cutoff_loss = torch.zeros(1).to('cuda')\n",
        "            ge_total_comp_loss = torch.zeros(1).to('cuda')\n",
        "\n",
        "            if reg_count % reg_frequency == 0:\n",
        "\n",
        "                # # compute in reg loss:\n",
        "                # # randomly generate 50 (N x 50) tensor --> 50 x N x 50\n",
        "                # cat_context_value_embed = clf.context_value_embeddings(in_lookup_tensor).expand(50, -1, -1)\n",
        "                # negative_samples = torch.randn(50, total_cat_nodes, embedding_dimension).to('cuda') * 0.001 # rand vs randn\n",
        "\n",
        "                # be_module_input = torch.cat([negative_samples, cat_context_value_embed], dim = 2)\n",
        "                # total_in_loss += torch.sum( -1.0 * torch.log( 1.0 - clf.be_module(be_module_input) ) )\n",
        "\n",
        "\n",
        "                # for i in be_Si_indexes:\n",
        "                #     feature_bin_embed = clf.num_feature_embeddings( i[0] )\n",
        "                #     feature_context_embed = clf.context_value_embeddings( i[1] )\n",
        "                #     be_module_input = torch.cat( [feature_bin_embed, feature_context_embed], axis = 1 )\n",
        "                #     be_module_output = clf.be_module(be_module_input)\n",
        "\n",
        "                #     # compute set reg loss:\n",
        "                #     total_be_loss += torch.sum( torch.min( torch.cat( [ (1.0 - be_module_output), (be_module_output - 0.0) ], axis = 1 ), dim = 1 )[0] )\n",
        "\n",
        "                #     # compute equal reg loss (less efficient; only to verify):\n",
        "                #     total_be_loss += torch.abs( 1.0 - torch.sum(be_module_output) )\n",
        "                #     total_be_loss += torch.abs( 1.0 - ( torch.max(be_module_output) - torch.min(be_module_output) ) )\n",
        "\n",
        "                # compute equal & set regs loss more efficiently:\n",
        "                for i in cat_features_Si_indexes:\n",
        "                    feature_bin_embed = clf.num_feature_embeddings( i[0] )\n",
        "                    feature_context_embed = clf.context_value_embeddings( i[1] )\n",
        "                    be_module_input = torch.cat( [feature_bin_embed, feature_context_embed], axis = 2 )\n",
        "                    be_module_output = clf.be_module(be_module_input)\n",
        "\n",
        "                    # compute set reg loss:\n",
        "                    total_be_loss += torch.sum( torch.min( torch.cat( [ (1.0 - be_module_output), (be_module_output - 0.0) ], axis = 2 ), dim = 2 )[0] )\n",
        "\n",
        "                    # # compute equal reg loss:\n",
        "                    # total_be_loss += torch.sum( torch.abs( 1.0 - torch.sum(be_module_output, dim = 1) ) )\n",
        "                    # total_be_loss += torch.sum( torch.abs( 1.0 - ( torch.max(be_module_output, 1)[0] - torch.min(be_module_output, 1)[0] ) ) )\n",
        "\n",
        "\n",
        "                # compute ordering regs:\n",
        "                if reg_count % (reg_frequency * 6) == 0:\n",
        "\n",
        "                    starting_index = random.randint(0, bins - 2)\n",
        "                    ending_index = random.randint(starting_index + 2, bins)\n",
        "\n",
        "                    for i in le_orderings_Si_indexes:\n",
        "                        feature_bin_embed = clf.num_feature_embeddings( i[0][starting_index:ending_index] )\n",
        "                        feature_context_embed = clf.context_value_embeddings( i[1][starting_index:ending_index] )\n",
        "                        num_module_input = torch.cat( [feature_bin_embed, feature_context_embed], axis = 1 )\n",
        "\n",
        "                        le_module_output = clf.le_module(num_module_input) # all values should <= first value\n",
        "                        le_total_ordering_loss += torch.sum( relu( le_module_output - le_module_output[0] ) )\n",
        "                        le_total_ordering_loss += torch.sum( relu( le_module_output[-1] - le_module_output ) )\n",
        "\n",
        "                        feature_bin_embed = clf.num_feature_embeddings( i[0] )\n",
        "                        feature_context_embed = clf.context_value_embeddings( i[1] )\n",
        "                        num_module_input_allbins = torch.cat( [feature_bin_embed, feature_context_embed], axis = 1 )\n",
        "\n",
        "                        le_module_output = clf.le_module(num_module_input_allbins)\n",
        "                        context_index = int( torch.argmin( torch.abs(le_module_output - 0.5) ) )\n",
        "                        le_total_cutoff_loss += torch.sum( relu( le_module_output[context_index] - le_module_output[:context_index] ) )\n",
        "                        le_total_cutoff_loss += torch.sum( relu( le_module_output[context_index + 1:] - le_module_output[context_index] ) )\n",
        "\n",
        "                    for i in ge_orderings_Si_indexes:\n",
        "                        feature_bin_embed = clf.num_feature_embeddings( i[0][starting_index:ending_index] )\n",
        "                        feature_context_embed = clf.context_value_embeddings( i[1][starting_index:ending_index] )\n",
        "                        num_module_input = torch.cat( [feature_bin_embed, feature_context_embed], axis = 1 )\n",
        "\n",
        "                        ge_module_output = clf.ge_module(num_module_input) # all values should <= last value\n",
        "                        ge_total_ordering_loss += torch.sum( relu( ge_module_output - ge_module_output[-1] ) )\n",
        "                        ge_total_ordering_loss += torch.sum( relu( ge_module_output[0] - ge_module_output ) )\n",
        "\n",
        "                        feature_bin_embed = clf.num_feature_embeddings( i[0] )\n",
        "                        feature_context_embed = clf.context_value_embeddings( i[1] )\n",
        "                        num_module_input_allbins = torch.cat( [feature_bin_embed, feature_context_embed], axis = 1 )\n",
        "\n",
        "                        ge_module_output = clf.ge_module(num_module_input_allbins)\n",
        "                        context_index = int( torch.argmin( torch.abs(ge_module_output - 0.5) ) )\n",
        "                        ge_total_cutoff_loss += torch.sum( relu( ge_module_output[context_index] - ge_module_output[context_index + 1:] ) )\n",
        "                        ge_total_cutoff_loss += torch.sum( relu( ge_module_output[:context_index] - ge_module_output[context_index] ) )\n",
        "\n",
        "\n",
        "                # compute le nodes ref & asym & trans reg losses 1st:\n",
        "                current_feature_bin_embed = clf.context_value_embeddings(le_ref_lookup_indexes)\n",
        "                feature_bin_input = torch.cat( [current_feature_bin_embed, current_feature_bin_embed], axis = 1 )\n",
        "\n",
        "                le_total_ref_loss += torch.sum( torch.abs( 0.5 - clf.le_module(feature_bin_input) ) )\n",
        "\n",
        "\n",
        "                for i in range(num_of_numerical_predictors):\n",
        "                    feature_bin_embed = clf.num_feature_embeddings( le_features_Si_indexes[i][0] )\n",
        "                    feature_context_embed = clf.context_value_embeddings( le_features_Si_indexes[i][1] )\n",
        "\n",
        "                    feature_all_embed = torch.cat( [feature_bin_embed, feature_context_embed], axis = 0 )\n",
        "                    feature_all_embed_1 = feature_all_embed[ torch.randperm(feature_all_embed.size()[0]) ]\n",
        "                    feature_all_embed_2 = feature_all_embed[ torch.randperm(feature_all_embed.size()[0]) ]\n",
        "\n",
        "                    ab_module_input = torch.cat( [feature_all_embed_1, feature_all_embed_2], axis = 1 )\n",
        "                    ba_module_input = torch.cat( [feature_all_embed_2, feature_all_embed_1], axis = 1 )\n",
        "\n",
        "                    le_total_asym_loss += torch.sum( torch.abs( clf.le_module(ab_module_input) - ( 1.0 - clf.le_module(ba_module_input) ) ) )\n",
        "\n",
        "\n",
        "                    feature_all_embed_1 = feature_all_embed[ torch.randperm(feature_all_embed.size()[0]) ]\n",
        "                    feature_all_embed_2 = feature_all_embed[ torch.randperm(feature_all_embed.size()[0]) ]\n",
        "                    feature_all_embed_3 = feature_all_embed[ torch.randperm(feature_all_embed.size()[0]) ]\n",
        "\n",
        "                    ab_module_input = torch.cat( [feature_all_embed_1, feature_all_embed_2], axis = 1 )\n",
        "                    bc_module_input = torch.cat( [feature_all_embed_2, feature_all_embed_3], axis = 1 )\n",
        "                    ac_module_input = torch.cat( [feature_all_embed_1, feature_all_embed_3], axis = 1 )\n",
        "\n",
        "                    ab_le_output = clf.le_module(ab_module_input)\n",
        "                    bc_le_output = clf.le_module(bc_module_input)\n",
        "                    ac_le_output = clf.le_module(ac_module_input)\n",
        "\n",
        "                    le_total_trans_loss += torch.sum( torch.sigmoid( (ab_le_output - 0.5) * 1e7 ) * torch.sigmoid( (bc_le_output - 0.5) * 1e7 ) * relu( torch.max(torch.cat( [ ab_le_output, bc_le_output ], axis = 1 ), 1)[0].view(-1, 1) - ac_le_output ) )\n",
        "                    le_total_trans_loss += torch.sum( torch.sigmoid( (0.5 - ab_le_output) * 1e7 ) * torch.sigmoid( (0.5 - bc_le_output) * 1e7 ) * relu( ac_le_output - torch.min(torch.cat( [ ab_le_output, bc_le_output ], axis = 1 ), 1)[0].view(-1, 1) ) )\n",
        "\n",
        "                    ba_module_input = torch.cat( [feature_all_embed_2, feature_all_embed_1], axis = 1 )\n",
        "                    cb_module_input = torch.cat( [feature_all_embed_3, feature_all_embed_2], axis = 1 )\n",
        "                    ca_module_input = torch.cat( [feature_all_embed_3, feature_all_embed_1], axis = 1 )\n",
        "\n",
        "                    ba_le_output = clf.le_module(ba_module_input)\n",
        "                    cb_le_output = clf.le_module(cb_module_input)\n",
        "                    ca_le_output = clf.le_module(ca_module_input)\n",
        "\n",
        "                    le_total_trans_loss += torch.sum( torch.sigmoid( (ba_le_output - 0.5) * 1e7 ) * torch.sigmoid( (cb_le_output - 0.5) * 1e7 ) * relu( torch.max(torch.cat( [ ba_le_output, cb_le_output ], axis = 1 ), 1)[0].view(-1, 1) - ca_le_output ) )\n",
        "                    le_total_trans_loss += torch.sum( torch.sigmoid( (0.5 - ba_le_output) * 1e7 ) * torch.sigmoid( (0.5 - cb_le_output) * 1e7 ) * relu( ca_le_output - torch.min(torch.cat( [ ba_le_output, cb_le_output ], axis = 1 ), 1)[0].view(-1, 1) ) )\n",
        "\n",
        "\n",
        "                    # proposed but dropped due to redundancy\n",
        "                    # feature_all_embed_1 = feature_all_embed[ torch.randperm(feature_all_embed.size()[0]) ]\n",
        "                    # feature_all_embed_2 = feature_all_embed[ torch.randperm(feature_all_embed.size()[0]) ]\n",
        "                    # feature_all_embed_3 = feature_all_embed[ torch.randperm(feature_all_embed.size()[0]) ]\n",
        "\n",
        "                    # ab_module_input = torch.cat( [feature_all_embed_1, feature_all_embed_2], axis = 1 )\n",
        "                    # bc_module_input = torch.cat( [feature_all_embed_2, feature_all_embed_3], axis = 1 )\n",
        "                    # ac_module_input = torch.cat( [feature_all_embed_1, feature_all_embed_3], axis = 1 )\n",
        "\n",
        "                    # ab_le_output = clf.le_module(ab_module_input)\n",
        "                    # bc_le_output = clf.le_module(bc_module_input)\n",
        "                    # ac_le_output = clf.le_module(ac_module_input)\n",
        "\n",
        "                    # diff = torch.abs(0.5 - ab_le_output) - torch.abs(0.5 - bc_le_output)\n",
        "                    # le_total_comp_loss += torch.sum( torch.sigmoid( (0.5 - ab_le_output) * 1e7 ) * torch.sigmoid( (bc_le_output - 0.5) * 1e7 ) * ( torch.sigmoid( ( torch.abs(0.5 - ab_le_output) - torch.abs(0.5 - bc_le_output) ) * 1e7 ) * torch.abs( ac_le_output - (0.5 - diff) ) + torch.sigmoid( ( torch.abs(0.5 - bc_le_output) - torch.abs(0.5 - ab_le_output) ) * 1e7 ) * torch.abs( (0.5 - diff) - ac_le_output ) ) )\n",
        "\n",
        "                    # diff = torch.abs(0.5 - bc_le_output) - torch.abs(0.5 - ab_le_output)\n",
        "                    # le_total_comp_loss += torch.sum( torch.sigmoid( (ab_le_output - 0.5) * 1e7 ) * torch.sigmoid( (0.5 - bc_le_output) * 1e7 ) * ( torch.sigmoid( ( torch.abs(0.5 - bc_le_output) - torch.abs(0.5 - ab_le_output) ) * 1e7 ) * torch.abs( ac_le_output - (0.5 - diff) ) + torch.sigmoid( ( torch.abs(0.5 - ab_le_output) - torch.abs(0.5 - bc_le_output) ) * 1e7 ) * torch.abs( (0.5 - diff) - ac_le_output ) ) )\n",
        "\n",
        "                    # ba_module_input = torch.cat( [feature_all_embed_2, feature_all_embed_1], axis = 1 )\n",
        "                    # cb_module_input = torch.cat( [feature_all_embed_3, feature_all_embed_2], axis = 1 )\n",
        "                    # ca_module_input = torch.cat( [feature_all_embed_3, feature_all_embed_1], axis = 1 )\n",
        "\n",
        "                    # ba_le_output = clf.le_module(ba_module_input)\n",
        "                    # cb_le_output = clf.le_module(cb_module_input)\n",
        "                    # ca_le_output = clf.le_module(ca_module_input)\n",
        "\n",
        "                    # diff = torch.abs(0.5 - ba_le_output) - torch.abs(0.5 - cb_le_output)\n",
        "                    # le_total_comp_loss += torch.sum( torch.sigmoid( (0.5 - ba_le_output) * 1e7 ) * torch.sigmoid( (cb_le_output - 0.5) * 1e7 ) * ( torch.sigmoid( ( torch.abs(0.5 - ba_le_output) - torch.abs(0.5 - cb_le_output) ) * 1e7 ) * torch.abs( ca_le_output - (0.5 - diff) ) + torch.sigmoid( ( torch.abs(0.5 - cb_le_output) - torch.abs(0.5 - ba_le_output) ) * 1e7 ) * torch.abs( (0.5 - diff) - ca_le_output ) ) )\n",
        "\n",
        "                    # diff = torch.abs(0.5 - cb_le_output) - torch.abs(0.5 - ba_le_output)\n",
        "                    # le_total_comp_loss += torch.sum( torch.sigmoid( (ba_le_output - 0.5) * 1e7 ) * torch.sigmoid( (0.5 - cb_le_output) * 1e7 ) * ( torch.sigmoid( ( torch.abs(0.5 - cb_le_output) - torch.abs(0.5 - ba_le_output) ) * 1e7 ) * torch.abs( ca_le_output - (0.5 - diff) ) + torch.sigmoid( ( torch.abs(0.5 - ba_le_output) - torch.abs(0.5 - cb_le_output) ) * 1e7 ) * torch.abs( (0.5 - diff) - ca_le_output ) ) )\n",
        "\n",
        "\n",
        "                # now compute ge nodes ref & asym & trans reg losses:\n",
        "                current_feature_bin_embed = clf.context_value_embeddings(ge_ref_lookup_indexes)\n",
        "                feature_bin_input = torch.cat( [current_feature_bin_embed, current_feature_bin_embed], axis = 1 )\n",
        "\n",
        "                ge_total_ref_loss += torch.sum( torch.abs( 0.5 - clf.ge_module(feature_bin_input) ) )\n",
        "\n",
        "\n",
        "                for i in range(num_of_numerical_predictors):\n",
        "                    feature_bin_embed = clf.num_feature_embeddings( ge_features_Si_indexes[i][0] )\n",
        "                    feature_context_embed = clf.context_value_embeddings( ge_features_Si_indexes[i][1] )\n",
        "\n",
        "                    feature_all_embed = torch.cat( [feature_bin_embed, feature_context_embed], axis = 0 )\n",
        "                    feature_all_embed_1 = feature_all_embed[ torch.randperm(feature_all_embed.size()[0]) ]\n",
        "                    feature_all_embed_2 = feature_all_embed[ torch.randperm(feature_all_embed.size()[0]) ]\n",
        "\n",
        "                    ab_module_input = torch.cat( [feature_all_embed_1, feature_all_embed_2], axis = 1 )\n",
        "                    ba_module_input = torch.cat( [feature_all_embed_2, feature_all_embed_1], axis = 1 )\n",
        "\n",
        "                    ge_total_asym_loss += torch.sum( torch.abs( clf.ge_module(ab_module_input) - ( 1.0 - clf.ge_module(ba_module_input) ) ) )\n",
        "\n",
        "\n",
        "                    feature_all_embed_1 = feature_all_embed[ torch.randperm(feature_all_embed.size()[0]) ]\n",
        "                    feature_all_embed_2 = feature_all_embed[ torch.randperm(feature_all_embed.size()[0]) ]\n",
        "                    feature_all_embed_3 = feature_all_embed[ torch.randperm(feature_all_embed.size()[0]) ]\n",
        "\n",
        "                    ab_module_input = torch.cat( [feature_all_embed_1, feature_all_embed_2], axis = 1 )\n",
        "                    bc_module_input = torch.cat( [feature_all_embed_2, feature_all_embed_3], axis = 1 )\n",
        "                    ac_module_input = torch.cat( [feature_all_embed_1, feature_all_embed_3], axis = 1 )\n",
        "\n",
        "                    ab_ge_output = clf.ge_module(ab_module_input)\n",
        "                    bc_ge_output = clf.ge_module(bc_module_input)\n",
        "                    ac_ge_output = clf.ge_module(ac_module_input)\n",
        "\n",
        "                    ge_total_trans_loss += torch.sum( torch.sigmoid( (ab_ge_output - 0.5) * 1e7 ) * torch.sigmoid( (bc_ge_output - 0.5) * 1e7 ) * relu( torch.max(torch.cat( [ ab_ge_output, bc_ge_output ], axis = 1 ), 1)[0].view(-1, 1) - ac_ge_output ) )\n",
        "                    ge_total_trans_loss += torch.sum( torch.sigmoid( (0.5 - ab_ge_output) * 1e7 ) * torch.sigmoid( (0.5 - bc_ge_output) * 1e7 ) * relu( ac_ge_output - torch.min(torch.cat( [ ab_ge_output, bc_ge_output ], axis = 1 ), 1)[0].view(-1, 1) ) )\n",
        "\n",
        "                    ba_module_input = torch.cat( [feature_all_embed_2, feature_all_embed_1], axis = 1 )\n",
        "                    cb_module_input = torch.cat( [feature_all_embed_3, feature_all_embed_2], axis = 1 )\n",
        "                    ca_module_input = torch.cat( [feature_all_embed_3, feature_all_embed_1], axis = 1 )\n",
        "\n",
        "                    ba_ge_output = clf.ge_module(ba_module_input)\n",
        "                    cb_ge_output = clf.ge_module(cb_module_input)\n",
        "                    ca_ge_output = clf.ge_module(ca_module_input)\n",
        "\n",
        "                    ge_total_trans_loss += torch.sum( torch.sigmoid( (ba_ge_output - 0.5) * 1e7 ) * torch.sigmoid( (cb_ge_output - 0.5) * 1e7 ) * relu( torch.max(torch.cat( [ ba_ge_output, cb_ge_output ], axis = 1 ), 1)[0].view(-1, 1) - ca_ge_output ) )\n",
        "                    ge_total_trans_loss += torch.sum( torch.sigmoid( (0.5 - ba_ge_output) * 1e7 ) * torch.sigmoid( (0.5 - cb_ge_output) * 1e7 ) * relu( ca_ge_output - torch.min(torch.cat( [ ba_ge_output, cb_ge_output ], axis = 1 ), 1)[0].view(-1, 1) ) )\n",
        "\n",
        "\n",
        "                    # proposed but dropped due to redundancy\n",
        "                    # feature_all_embed_1 = feature_all_embed[ torch.randperm(feature_all_embed.size()[0]) ]\n",
        "                    # feature_all_embed_2 = feature_all_embed[ torch.randperm(feature_all_embed.size()[0]) ]\n",
        "                    # feature_all_embed_3 = feature_all_embed[ torch.randperm(feature_all_embed.size()[0]) ]\n",
        "\n",
        "                    # ab_module_input = torch.cat( [feature_all_embed_1, feature_all_embed_2], axis = 1 )\n",
        "                    # bc_module_input = torch.cat( [feature_all_embed_2, feature_all_embed_3], axis = 1 )\n",
        "                    # ac_module_input = torch.cat( [feature_all_embed_1, feature_all_embed_3], axis = 1 )\n",
        "\n",
        "                    # ab_ge_output = clf.ge_module(ab_module_input)\n",
        "                    # bc_ge_output = clf.ge_module(bc_module_input)\n",
        "                    # ac_ge_output = clf.ge_module(ac_module_input)\n",
        "\n",
        "                    # diff = torch.abs(0.5 - ab_ge_output) - torch.abs(0.5 - bc_ge_output)\n",
        "                    # ge_total_comp_loss += torch.sum( torch.sigmoid( (0.5 - ab_ge_output) * 1e7 ) * torch.sigmoid( (bc_ge_output - 0.5) * 1e7 ) * ( torch.sigmoid( ( torch.abs(0.5 - ab_ge_output) - torch.abs(0.5 - bc_ge_output) ) * 1e7 ) * torch.abs( ac_ge_output - (0.5 - diff) ) + torch.sigmoid( ( torch.abs(0.5 - bc_ge_output) - torch.abs(0.5 - ab_ge_output) ) * 1e7 ) * torch.abs( (0.5 - diff) - ac_ge_output ) ) )\n",
        "\n",
        "                    # diff = torch.abs(0.5 - bc_ge_output) - torch.abs(0.5 - ab_ge_output)\n",
        "                    # ge_total_comp_loss += torch.sum( torch.sigmoid( (ab_ge_output - 0.5) * 1e7 ) * torch.sigmoid( (0.5 - bc_ge_output) * 1e7 ) * ( torch.sigmoid( ( torch.abs(0.5 - bc_ge_output) - torch.abs(0.5 - ab_ge_output) ) * 1e7 ) * torch.abs( ac_ge_output - (0.5 - diff) ) + torch.sigmoid( ( torch.abs(0.5 - ab_ge_output) - torch.abs(0.5 - bc_ge_output) ) * 1e7 ) * torch.abs( (0.5 - diff) - ac_ge_output ) ) )\n",
        "\n",
        "                    # ba_module_input = torch.cat( [feature_all_embed_2, feature_all_embed_1], axis = 1 )\n",
        "                    # cb_module_input = torch.cat( [feature_all_embed_3, feature_all_embed_2], axis = 1 )\n",
        "                    # ca_module_input = torch.cat( [feature_all_embed_3, feature_all_embed_1], axis = 1 )\n",
        "\n",
        "                    # ba_ge_output = clf.ge_module(ba_module_input)\n",
        "                    # cb_ge_output = clf.ge_module(cb_module_input)\n",
        "                    # ca_ge_output = clf.ge_module(ca_module_input)\n",
        "\n",
        "                    # diff = torch.abs(0.5 - ba_ge_output) - torch.abs(0.5 - cb_ge_output)\n",
        "                    # ge_total_comp_loss += torch.sum( torch.sigmoid( (0.5 - ba_ge_output) * 1e7 ) * torch.sigmoid( (cb_ge_output - 0.5) * 1e7 ) * ( torch.sigmoid( ( torch.abs(0.5 - ba_ge_output) - torch.abs(0.5 - cb_ge_output) ) * 1e7 ) * torch.abs( ca_ge_output - (0.5 - diff) ) + torch.sigmoid( ( torch.abs(0.5 - cb_ge_output) - torch.abs(0.5 - ba_ge_output) ) * 1e7 ) * torch.abs( (0.5 - diff) - ca_ge_output ) ) )\n",
        "\n",
        "                    # diff = torch.abs(0.5 - bc_ge_output) - torch.abs(0.5 - ab_ge_output)\n",
        "                    # ge_total_comp_loss += torch.sum( torch.sigmoid( (ba_ge_output - 0.5) * 1e7 ) * torch.sigmoid( (0.5 - cb_ge_output) * 1e7 ) * ( torch.sigmoid( ( torch.abs(0.5 - cb_ge_output) - torch.abs(0.5 - ba_ge_output) ) * 1e7 ) * torch.abs( ca_ge_output - (0.5 - diff) ) + torch.sigmoid( ( torch.abs(0.5 - ba_ge_output) - torch.abs(0.5 - cb_ge_output) ) * 1e7 ) * torch.abs( (0.5 - diff) - ca_ge_output ) ) )\n",
        "\n",
        "\n",
        "                # compute new le & ge reg loss w/ static bin diff.:\n",
        "                for i in range(bins):\n",
        "                    current_feature_bin_embed = clf.num_feature_embeddings( le_current_feature_indexes[i] )\n",
        "                    next_feature_bin_embed = clf.num_feature_embeddings( le_next_feature_indexes[i] )\n",
        "\n",
        "                    feature_bin_input_1 = torch.cat( [current_feature_bin_embed, next_feature_bin_embed], axis = 2 )\n",
        "                    feature_bin_input_2 = torch.cat( [next_feature_bin_embed, current_feature_bin_embed], axis = 2 )\n",
        "\n",
        "                    total_le_loss += torch.sum( torch.abs( le_expected_outputs_1[i] - torch.squeeze( clf.le_module(feature_bin_input_1) ) ) )\n",
        "                    total_le_loss += torch.sum( torch.abs( le_expected_outputs_2[i] - torch.squeeze( clf.le_module(feature_bin_input_2) ) ) )\n",
        "\n",
        "\n",
        "                    current_feature_bin_embed = clf.num_feature_embeddings( ge_current_feature_indexes[i] )\n",
        "                    next_feature_bin_embed = clf.num_feature_embeddings( ge_next_feature_indexes[i] )\n",
        "\n",
        "                    feature_bin_input_1 = torch.cat( [current_feature_bin_embed, next_feature_bin_embed], axis = 2 )\n",
        "                    feature_bin_input_2 = torch.cat( [next_feature_bin_embed, current_feature_bin_embed], axis = 2 )\n",
        "\n",
        "                    total_ge_loss += torch.sum( torch.abs( ge_expected_outputs_1[i] - torch.squeeze( clf.ge_module(feature_bin_input_1) ) ) )\n",
        "                    total_ge_loss += torch.sum( torch.abs( ge_expected_outputs_2[i] - torch.squeeze( clf.ge_module(feature_bin_input_2) ) ) )\n",
        "\n",
        "\n",
        "                # batch_reweight = reg_weight * reg_frequency * ( 1.0/ batch_size )\n",
        "\n",
        "                le_batch_reweight = le_reg_weight * reg_frequency *  ( 1.0/ batch_size )\n",
        "                ge_batch_reweight = ge_reg_weight * reg_frequency *  ( 1.0/ batch_size )\n",
        "                be_batch_reweight = be_weight * be_reg_weight * reg_frequency *  ( 1.0/ batch_size )\n",
        "\n",
        "                loss = bce_loss(preds, target) + be_batch_reweight * total_be_loss + le_batch_reweight * (total_le_loss + le_total_ref_loss + le_total_asym_loss + le_total_trans_loss + 10.0 * le_total_ordering_loss + 10.0 * le_total_cutoff_loss) + ge_batch_reweight * (total_ge_loss + ge_total_ref_loss + ge_total_asym_loss + ge_total_trans_loss + 10.0 * ge_total_ordering_loss + 10.0 * ge_total_cutoff_loss)\n",
        "\n",
        "                if (epoch == 0 and reg_count == 0):\n",
        "                    init_num_regs_losses = (total_le_loss + total_ge_loss + le_total_ref_loss + le_total_asym_loss + le_total_trans_loss + 10.0 * le_total_ordering_loss + 10.0 * le_total_cutoff_loss + ge_total_ref_loss + ge_total_asym_loss + ge_total_trans_loss + 10.0 * ge_total_ordering_loss + 10.0 * ge_total_cutoff_loss) # + le_total_comp_loss + ge_total_comp_loss\n",
        "                    init_cat_regs_losses = be_weight * total_be_loss\n",
        "\n",
        "                if (epoch != 0):\n",
        "                    end_num_regs_losses = (total_le_loss + total_ge_loss + le_total_ref_loss + le_total_asym_loss + le_total_trans_loss + 10.0 * le_total_ordering_loss + 10.0 * le_total_cutoff_loss + ge_total_ref_loss + ge_total_asym_loss + ge_total_trans_loss + 10.0 * ge_total_ordering_loss + 10.0 * ge_total_cutoff_loss) # + le_total_comp_loss + ge_total_comp_loss\n",
        "                    end_cat_regs_losses = be_weight * total_be_loss\n",
        "\n",
        "                # print(loss)\n",
        "                # le_reg_weight = le_reg_weight * ( 1.0/ batch_size )\n",
        "                # ge_reg_weight = ge_reg_weight * ( 1.0/ batch_size )\n",
        "                # be_reg_weight = be_reg_weight * ( 1.0/ batch_size )\n",
        "                # ordering_reg_weight = max(le_reg_weight, ge_reg_weight, be_reg_weight)\n",
        "\n",
        "                # loss = bce_loss(preds, target) + le_reg_weight * (total_le_loss + le_total_ref_loss + le_total_asym_loss + le_total_trans_loss) + ge_reg_weight * (total_ge_loss + ge_total_ref_loss + ge_total_asym_loss + ge_total_trans_loss) + be_reg_weight * total_be_loss + ordering_reg_weight * (100.0 * le_total_ordering_loss + 30.0 * le_total_cutoff_loss + 100.0 * ge_total_ordering_loss + 30.0 * ge_total_cutoff_loss)\n",
        "                # print(loss)\n",
        "                # print()\n",
        "\n",
        "\n",
        "                if reg_count == 0:\n",
        "                    print('total_le_loss: ', total_le_loss)\n",
        "                    print('total_ge_loss: ', total_ge_loss)\n",
        "                    print('total_be_loss: ', total_be_loss)\n",
        "                    print('le_total_ref_loss: ', le_total_ref_loss)\n",
        "                    print('le_total_asym_loss: ', le_total_asym_loss)\n",
        "                    print('le_total_trans_loss: ', le_total_trans_loss)\n",
        "                    print('le_total_ordering_loss: ', le_total_ordering_loss)\n",
        "                    print('le_total_cutoff_loss: ', le_total_cutoff_loss)\n",
        "                    print('le_total_comp_loss: ', le_total_comp_loss)\n",
        "                    print('ge_total_ref_loss: ', ge_total_ref_loss)\n",
        "                    print('ge_total_asym_loss: ', ge_total_asym_loss)\n",
        "                    print('ge_total_trans_loss: ', ge_total_trans_loss)\n",
        "                    print('ge_total_ordering_loss: ', ge_total_ordering_loss)\n",
        "                    print('ge_total_cutoff_loss: ', ge_total_cutoff_loss)\n",
        "                    print('ge_total_comp_loss: ', ge_total_comp_loss)\n",
        "\n",
        "\n",
        "            else:\n",
        "                loss = bce_loss(preds, target)\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            clf.zero_grad()\n",
        "\n",
        "            total_loss += loss\n",
        "            batch_count += 1\n",
        "            reg_count += 1\n",
        "\n",
        "\n",
        "        # perform early-stopping:\n",
        "        clf.NSDT_masking = False\n",
        "        clf = clf.eval()\n",
        "\n",
        "        # Create context value index lists for training set:\n",
        "        val_all_batch_le_context_value_index_list = []\n",
        "        val_all_batch_ge_context_value_index_list = []\n",
        "        val_all_batch_be_context_value_index_list = []\n",
        "\n",
        "        for batch in val_loader:\n",
        "\n",
        "            val_batch_size = len(batch)\n",
        "            val_batch_le_context_value_index_list = []\n",
        "            val_batch_ge_context_value_index_list = []\n",
        "            val_batch_be_context_value_index_list = []\n",
        "\n",
        "            for node in le_nodes:\n",
        "                le_context_value_index_list = [ node[2] ] * val_batch_size\n",
        "                val_batch_le_context_value_index_list.append(le_context_value_index_list)\n",
        "\n",
        "            for node in ge_nodes:\n",
        "                ge_context_value_index_list = [ node[2] ] * val_batch_size\n",
        "                val_batch_ge_context_value_index_list.append(ge_context_value_index_list)\n",
        "\n",
        "            for node in be_nodes:\n",
        "                be_context_value_index_list = [ node[2] ] * val_batch_size\n",
        "                val_batch_be_context_value_index_list.append(be_context_value_index_list)\n",
        "\n",
        "            val_all_batch_le_context_value_index_list.append(val_batch_le_context_value_index_list)\n",
        "            val_all_batch_ge_context_value_index_list.append(val_batch_ge_context_value_index_list)\n",
        "            val_all_batch_be_context_value_index_list.append(val_batch_be_context_value_index_list)\n",
        "\n",
        "\n",
        "        val_pred_labels = []\n",
        "\n",
        "        batch_len = 0\n",
        "\n",
        "        val_feature_indexes_le_nodes  = []\n",
        "        val_feature_indexes_ge_nodes  = []\n",
        "        val_feature_indexes_be_nodes  = []\n",
        "\n",
        "        for node in le_nodes:\n",
        "            val_feature_indexes_le_nodes.append(node[0] - 1)\n",
        "\n",
        "        for node in ge_nodes:\n",
        "            val_feature_indexes_ge_nodes.append(node[0] - 1)\n",
        "\n",
        "        for node in be_nodes:\n",
        "            val_feature_indexes_be_nodes.append(node[0] - 1)\n",
        "\n",
        "\n",
        "        for batch in val_loader:\n",
        "\n",
        "            subset = batch[:, :] # X only; target already removed\n",
        "            val_batch_size = len(batch)\n",
        "\n",
        "            val_batch_le_feature_index_list = subset[:, val_feature_indexes_le_nodes].permute(1,0).to('cuda')\n",
        "            val_batch_ge_feature_index_list = subset[:, val_feature_indexes_ge_nodes].permute(1,0).to('cuda')\n",
        "            val_batch_be_feature_index_list = subset[:, val_feature_indexes_be_nodes].permute(1,0).to('cuda')\n",
        "\n",
        "            val_batch_le_context_value_index_list = val_all_batch_le_context_value_index_list[batch_len]\n",
        "            val_batch_ge_context_value_index_list = val_all_batch_ge_context_value_index_list[batch_len]\n",
        "            val_batch_be_context_value_index_list = val_all_batch_be_context_value_index_list[batch_len]\n",
        "\n",
        "            val_preds = clf.forward(val_batch_le_feature_index_list, val_batch_le_context_value_index_list,\n",
        "                                val_batch_ge_feature_index_list, val_batch_ge_context_value_index_list,\n",
        "                                val_batch_be_feature_index_list, val_batch_be_context_value_index_list,\n",
        "                                val_batch_size )\n",
        "\n",
        "\n",
        "            val_preds = torch.sigmoid(val_preds)\n",
        "            val_preds = torch.squeeze(val_preds).tolist()\n",
        "\n",
        "            batch_len += 1\n",
        "\n",
        "            for i in val_preds:\n",
        "                if i > 0.5:\n",
        "                    val_pred_labels.append(1)\n",
        "                if i <= 0.5:\n",
        "                    val_pred_labels.append(0)\n",
        "\n",
        "        # Compute Balanced Accuracy:\n",
        "        val_accuracy = metrics.balanced_accuracy_score(val_data['target'], val_pred_labels)\n",
        "\n",
        "\n",
        "        # now compute test accuracy for bayes optimization record\n",
        "        # Create context value index lists for training set:\n",
        "        test_all_batch_le_context_value_index_list = []\n",
        "        test_all_batch_ge_context_value_index_list = []\n",
        "        test_all_batch_be_context_value_index_list = []\n",
        "\n",
        "        for batch in test_loader:\n",
        "\n",
        "            test_batch_size = len(batch)\n",
        "            test_batch_le_context_value_index_list = []\n",
        "            test_batch_ge_context_value_index_list = []\n",
        "            test_batch_be_context_value_index_list = []\n",
        "\n",
        "            for node in le_nodes:\n",
        "                le_context_value_index_list = [ node[2] ] * test_batch_size\n",
        "                test_batch_le_context_value_index_list.append(le_context_value_index_list)\n",
        "\n",
        "            for node in ge_nodes:\n",
        "                ge_context_value_index_list = [ node[2] ] * test_batch_size\n",
        "                test_batch_ge_context_value_index_list.append(ge_context_value_index_list)\n",
        "\n",
        "            for node in be_nodes:\n",
        "                be_context_value_index_list = [ node[2] ] * test_batch_size\n",
        "                test_batch_be_context_value_index_list.append(be_context_value_index_list)\n",
        "\n",
        "            test_all_batch_le_context_value_index_list.append(test_batch_le_context_value_index_list)\n",
        "            test_all_batch_ge_context_value_index_list.append(test_batch_ge_context_value_index_list)\n",
        "            test_all_batch_be_context_value_index_list.append(test_batch_be_context_value_index_list)\n",
        "\n",
        "\n",
        "        test_pred_labels = []\n",
        "\n",
        "        batch_len = 0\n",
        "\n",
        "        test_feature_indexes_le_nodes  = []\n",
        "        test_feature_indexes_ge_nodes  = []\n",
        "        test_feature_indexes_be_nodes  = []\n",
        "\n",
        "        for node in le_nodes:\n",
        "            test_feature_indexes_le_nodes.append(node[0] - 1)\n",
        "\n",
        "        for node in ge_nodes:\n",
        "            test_feature_indexes_ge_nodes.append(node[0] - 1)\n",
        "\n",
        "        for node in be_nodes:\n",
        "            test_feature_indexes_be_nodes.append(node[0] - 1)\n",
        "\n",
        "\n",
        "        for batch in test_loader:\n",
        "\n",
        "            subset = batch[:, :] # X only; target already removed\n",
        "            test_batch_size = len(batch)\n",
        "\n",
        "            test_batch_le_feature_index_list = subset[:, test_feature_indexes_le_nodes].permute(1,0).to('cuda')\n",
        "            test_batch_ge_feature_index_list = subset[:, test_feature_indexes_ge_nodes].permute(1,0).to('cuda')\n",
        "            test_batch_be_feature_index_list = subset[:, test_feature_indexes_be_nodes].permute(1,0).to('cuda')\n",
        "\n",
        "            test_batch_le_context_value_index_list = test_all_batch_le_context_value_index_list[batch_len]\n",
        "            test_batch_ge_context_value_index_list = test_all_batch_ge_context_value_index_list[batch_len]\n",
        "            test_batch_be_context_value_index_list = test_all_batch_be_context_value_index_list[batch_len]\n",
        "\n",
        "            test_preds = clf.forward(test_batch_le_feature_index_list, test_batch_le_context_value_index_list,\n",
        "                                test_batch_ge_feature_index_list, test_batch_ge_context_value_index_list,\n",
        "                                test_batch_be_feature_index_list, test_batch_be_context_value_index_list,\n",
        "                                test_batch_size )\n",
        "\n",
        "\n",
        "            test_preds = torch.sigmoid(test_preds)\n",
        "            test_preds = torch.squeeze(test_preds).tolist()\n",
        "\n",
        "            batch_len += 1\n",
        "\n",
        "            for i in test_preds:\n",
        "                if i > 0.5:\n",
        "                    test_pred_labels.append(1)\n",
        "                if i <= 0.5:\n",
        "                    test_pred_labels.append(0)\n",
        "\n",
        "        # Compute Balanced Accuracy:\n",
        "        test_accuracy = metrics.balanced_accuracy_score(test_data['target'], test_pred_labels)\n",
        "\n",
        "        clf = clf.train()\n",
        "        clf.NSDT_masking = masking\n",
        "\n",
        "        accs.append( [val_accuracy, test_accuracy] )\n",
        "\n",
        "        if early_stopper.early_stop(-val_accuracy):\n",
        "            if dataset == 'Higgs':\n",
        "                return clf, accs[ len(accs) - early_stopper.patience - 1 ][0], accs[ len(accs) - early_stopper.patience - 1 ][1], init_num_regs_losses, end_num_regs_losses, init_cat_regs_losses, end_cat_regs_losses\n",
        "            else:\n",
        "                return clf, accs[ len(accs) - 1 ][0], accs[ len(accs) - 1 ][1], init_num_regs_losses, end_num_regs_losses, init_cat_regs_losses, end_cat_regs_losses\n",
        "\n",
        "        print()\n",
        "        print( epoch + 1, ': ', 'total loss: ', total_loss, '; val accuracy: ', accs[epoch][0] )\n",
        "\n",
        "    return clf, accs[ len(accs) - 1 ][0], accs[ len(accs) - 1 ][1], init_num_regs_losses, end_num_regs_losses, init_cat_regs_losses, end_cat_regs_losses\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Lig_wrIMhKMB"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "GEXTQA-3hKjJ"
      },
      "outputs": [],
      "source": [
        "def Train_num_feature_only_NSDT_split_regs(masking, train_loader, val_loader, test_loader, early_stopper, new_train, cut_off_dict, feature_lookup_dict, cat_bins_total, total_numerical_nodes, total_levels, bins, embedding_dimension, n_iter, mlp_optimizer, hidden_init, hidden_nodes_layer1, hidden_nodes_layer2, activation, learning_rate, weight_decays, reg_frequency, le_reg_weight, ge_reg_weight):\n",
        "\n",
        "    feature_indexes_le_nodes, feature_indexes_ge_nodes, feature_indexes_be_nodes, all_batch_le_context_value_index_list, all_batch_ge_context_value_index_list, all_batch_be_context_value_index_list, le_orderings_Si_indexes, ge_orderings_Si_indexes, le_ref_lookup_indexes, le_features_Si_indexes, ge_ref_lookup_indexes, ge_features_Si_indexes, le_current_feature_indexes, le_next_feature_indexes, le_expected_outputs_1, le_expected_outputs_2, ge_current_feature_indexes, ge_next_feature_indexes, ge_expected_outputs_1, ge_expected_outputs_2 = NSDT_modeling_preps(bins, train_loader, new_train, cut_off_dict, feature_lookup_dict, cat_bins_total, total_numerical_nodes, total_levels)\n",
        "\n",
        "    clf = NSDT_num_feature_only(masking, total_levels, embedding_dimension, hidden_init, hidden_nodes_layer1, hidden_nodes_layer2, activation).to('cuda')\n",
        "\n",
        "    relu = torch.nn.ReLU()\n",
        "\n",
        "    pos_weight = torch.tensor( [training_reweight] ).to('cuda')\n",
        "    bce_loss = nn.BCEWithLogitsLoss(pos_weight = pos_weight, reduction = 'mean')\n",
        "\n",
        "    if (mlp_optimizer == 'adam'):\n",
        "        optimizer = optim.Adam(clf.parameters(), lr = learning_rate, weight_decay = weight_decays)\n",
        "    else:\n",
        "        optimizer = optim.AdamW(clf.parameters(), lr = learning_rate, weight_decay = weight_decays)\n",
        "\n",
        "    print('Start Training:')\n",
        "    print()\n",
        "\n",
        "    accs = []\n",
        "\n",
        "    for epoch in range(n_iter):\n",
        "\n",
        "        total_loss = 0.0\n",
        "        batch_count = 0\n",
        "        reg_count = 0\n",
        "\n",
        "        for batch in train_loader:\n",
        "\n",
        "            subset = batch[:,:num_of_predictors] # X\n",
        "            batch_size = len(batch)\n",
        "\n",
        "            target = batch[:,num_of_predictors].to('cuda') # y\n",
        "            target = target.float()\n",
        "\n",
        "            batch_le_feature_index_list = subset[:, feature_indexes_le_nodes].permute(1,0).to('cuda')\n",
        "            batch_ge_feature_index_list = subset[:, feature_indexes_ge_nodes].permute(1,0).to('cuda')\n",
        "\n",
        "            batch_le_context_value_index_list = all_batch_le_context_value_index_list[batch_count]\n",
        "            batch_ge_context_value_index_list = all_batch_ge_context_value_index_list[batch_count]\n",
        "\n",
        "            preds = clf.forward(batch_le_feature_index_list, batch_le_context_value_index_list,\n",
        "                                batch_ge_feature_index_list, batch_ge_context_value_index_list,\n",
        "                                batch_size)\n",
        "            preds = torch.squeeze(preds)\n",
        "\n",
        "\n",
        "            # add regularizers losses once every 5 batch:\n",
        "            total_le_loss = torch.zeros(1).to('cuda')\n",
        "            total_ge_loss = torch.zeros(1).to('cuda')\n",
        "\n",
        "            le_total_ref_loss = torch.zeros(1).to('cuda')\n",
        "            le_total_asym_loss = torch.zeros(1).to('cuda')\n",
        "            le_total_trans_loss = torch.zeros(1).to('cuda')\n",
        "            le_total_ordering_loss = torch.zeros(1).to('cuda')\n",
        "            le_total_cutoff_loss = torch.zeros(1).to('cuda')\n",
        "            le_total_comp_loss = torch.zeros(1).to('cuda')\n",
        "\n",
        "            ge_total_ref_loss = torch.zeros(1).to('cuda')\n",
        "            ge_total_asym_loss = torch.zeros(1).to('cuda')\n",
        "            ge_total_trans_loss = torch.zeros(1).to('cuda')\n",
        "            ge_total_ordering_loss = torch.zeros(1).to('cuda')\n",
        "            ge_total_cutoff_loss = torch.zeros(1).to('cuda')\n",
        "            ge_total_comp_loss = torch.zeros(1).to('cuda')\n",
        "\n",
        "            if reg_count % reg_frequency == 0:\n",
        "\n",
        "                # compute ordering regs:\n",
        "                if reg_count % (reg_frequency * 3) == 0:\n",
        "\n",
        "                    starting_index = random.randint(0, bins - 2)\n",
        "                    ending_index = random.randint(starting_index + 2, bins)\n",
        "\n",
        "                    for i in le_orderings_Si_indexes:\n",
        "                        feature_bin_embed = clf.num_feature_embeddings( i[0][starting_index:ending_index] )\n",
        "                        feature_context_embed = clf.context_value_embeddings( i[1][starting_index:ending_index] )\n",
        "                        num_module_input = torch.cat( [feature_bin_embed, feature_context_embed], axis = 1 )\n",
        "\n",
        "                        le_module_output = clf.le_module(num_module_input) # all values should <= first value\n",
        "                        le_total_ordering_loss += torch.sum( relu( le_module_output - le_module_output[0] ) )\n",
        "                        le_total_ordering_loss += torch.sum( relu( le_module_output[-1] - le_module_output ) )\n",
        "\n",
        "                        feature_bin_embed = clf.num_feature_embeddings( i[0] )\n",
        "                        feature_context_embed = clf.context_value_embeddings( i[1] )\n",
        "                        num_module_input_allbins = torch.cat( [feature_bin_embed, feature_context_embed], axis = 1 )\n",
        "\n",
        "                        le_module_output = clf.le_module(num_module_input_allbins)\n",
        "                        context_index = int( torch.argmin( torch.abs(le_module_output - 0.5) ) )\n",
        "                        le_total_cutoff_loss += torch.sum( relu( le_module_output[context_index] - le_module_output[:context_index] ) )\n",
        "                        le_total_cutoff_loss += torch.sum( relu( le_module_output[context_index + 1:] - le_module_output[context_index] ) )\n",
        "\n",
        "                    for i in ge_orderings_Si_indexes:\n",
        "                        feature_bin_embed = clf.num_feature_embeddings( i[0][starting_index:ending_index] )\n",
        "                        feature_context_embed = clf.context_value_embeddings( i[1][starting_index:ending_index] )\n",
        "                        num_module_input = torch.cat( [feature_bin_embed, feature_context_embed], axis = 1 )\n",
        "\n",
        "                        ge_module_output = clf.ge_module(num_module_input) # all values should <= last value\n",
        "                        ge_total_ordering_loss += torch.sum( relu( ge_module_output - ge_module_output[-1] ) )\n",
        "                        ge_total_ordering_loss += torch.sum( relu( ge_module_output[0] - ge_module_output ) )\n",
        "\n",
        "                        feature_bin_embed = clf.num_feature_embeddings( i[0] )\n",
        "                        feature_context_embed = clf.context_value_embeddings( i[1] )\n",
        "                        num_module_input_allbins = torch.cat( [feature_bin_embed, feature_context_embed], axis = 1 )\n",
        "\n",
        "                        ge_module_output = clf.ge_module(num_module_input_allbins)\n",
        "                        context_index = int( torch.argmin( torch.abs(ge_module_output - 0.5) ) )\n",
        "                        ge_total_cutoff_loss += torch.sum( relu( ge_module_output[context_index] - ge_module_output[context_index + 1:] ) )\n",
        "                        ge_total_cutoff_loss += torch.sum( relu( ge_module_output[:context_index] - ge_module_output[context_index] ) )\n",
        "\n",
        "\n",
        "                # compute le nodes ref & asym & trans reg losses 1st:\n",
        "                current_feature_bin_embed = clf.context_value_embeddings(le_ref_lookup_indexes)\n",
        "                feature_bin_input = torch.cat( [current_feature_bin_embed, current_feature_bin_embed], axis = 1 )\n",
        "\n",
        "                le_total_ref_loss += torch.sum( torch.abs( 0.5 - clf.le_module(feature_bin_input) ) )\n",
        "\n",
        "\n",
        "                for i in range(num_of_numerical_predictors):\n",
        "                    feature_bin_embed = clf.num_feature_embeddings( le_features_Si_indexes[i][0] )\n",
        "                    feature_context_embed = clf.context_value_embeddings( le_features_Si_indexes[i][1] )\n",
        "\n",
        "                    feature_all_embed = torch.cat( [feature_bin_embed, feature_context_embed], axis = 0 )\n",
        "                    feature_all_embed_1 = feature_all_embed[ torch.randperm(feature_all_embed.size()[0]) ]\n",
        "                    feature_all_embed_2 = feature_all_embed[ torch.randperm(feature_all_embed.size()[0]) ]\n",
        "\n",
        "                    ab_module_input = torch.cat( [feature_all_embed_1, feature_all_embed_2], axis = 1 )\n",
        "                    ba_module_input = torch.cat( [feature_all_embed_2, feature_all_embed_1], axis = 1 )\n",
        "\n",
        "                    le_total_asym_loss += torch.sum( torch.abs( clf.le_module(ab_module_input) - ( 1.0 - clf.le_module(ba_module_input) ) ) )\n",
        "\n",
        "\n",
        "                    feature_all_embed_1 = feature_all_embed[ torch.randperm(feature_all_embed.size()[0]) ]\n",
        "                    feature_all_embed_2 = feature_all_embed[ torch.randperm(feature_all_embed.size()[0]) ]\n",
        "                    feature_all_embed_3 = feature_all_embed[ torch.randperm(feature_all_embed.size()[0]) ]\n",
        "\n",
        "                    ab_module_input = torch.cat( [feature_all_embed_1, feature_all_embed_2], axis = 1 )\n",
        "                    bc_module_input = torch.cat( [feature_all_embed_2, feature_all_embed_3], axis = 1 )\n",
        "                    ac_module_input = torch.cat( [feature_all_embed_1, feature_all_embed_3], axis = 1 )\n",
        "\n",
        "                    ab_le_output = clf.le_module(ab_module_input)\n",
        "                    bc_le_output = clf.le_module(bc_module_input)\n",
        "                    ac_le_output = clf.le_module(ac_module_input)\n",
        "\n",
        "                    le_total_trans_loss += torch.sum( torch.sigmoid( (ab_le_output - 0.5) * 1e7 ) * torch.sigmoid( (bc_le_output - 0.5) * 1e7 ) * relu( torch.max(torch.cat( [ ab_le_output, bc_le_output ], axis = 1 ), 1)[0].view(-1, 1) - ac_le_output ) )\n",
        "                    le_total_trans_loss += torch.sum( torch.sigmoid( (0.5 - ab_le_output) * 1e7 ) * torch.sigmoid( (0.5 - bc_le_output) * 1e7 ) * relu( ac_le_output - torch.min(torch.cat( [ ab_le_output, bc_le_output ], axis = 1 ), 1)[0].view(-1, 1) ) )\n",
        "\n",
        "                    ba_module_input = torch.cat( [feature_all_embed_2, feature_all_embed_1], axis = 1 )\n",
        "                    cb_module_input = torch.cat( [feature_all_embed_3, feature_all_embed_2], axis = 1 )\n",
        "                    ca_module_input = torch.cat( [feature_all_embed_3, feature_all_embed_1], axis = 1 )\n",
        "\n",
        "                    ba_le_output = clf.le_module(ba_module_input)\n",
        "                    cb_le_output = clf.le_module(cb_module_input)\n",
        "                    ca_le_output = clf.le_module(ca_module_input)\n",
        "\n",
        "                    le_total_trans_loss += torch.sum( torch.sigmoid( (ba_le_output - 0.5) * 1e7 ) * torch.sigmoid( (cb_le_output - 0.5) * 1e7 ) * relu( torch.max(torch.cat( [ ba_le_output, cb_le_output ], axis = 1 ), 1)[0].view(-1, 1) - ca_le_output ) )\n",
        "                    le_total_trans_loss += torch.sum( torch.sigmoid( (0.5 - ba_le_output) * 1e7 ) * torch.sigmoid( (0.5 - cb_le_output) * 1e7 ) * relu( ca_le_output - torch.min(torch.cat( [ ba_le_output, cb_le_output ], axis = 1 ), 1)[0].view(-1, 1) ) )\n",
        "\n",
        "\n",
        "                    # feature_all_embed_1 = feature_all_embed[ torch.randperm(feature_all_embed.size()[0]) ]\n",
        "                    # feature_all_embed_2 = feature_all_embed[ torch.randperm(feature_all_embed.size()[0]) ]\n",
        "                    # feature_all_embed_3 = feature_all_embed[ torch.randperm(feature_all_embed.size()[0]) ]\n",
        "\n",
        "                    # ab_module_input = torch.cat( [feature_all_embed_1, feature_all_embed_2], axis = 1 )\n",
        "                    # bc_module_input = torch.cat( [feature_all_embed_2, feature_all_embed_3], axis = 1 )\n",
        "                    # ac_module_input = torch.cat( [feature_all_embed_1, feature_all_embed_3], axis = 1 )\n",
        "\n",
        "                    # ab_le_output = clf.le_module(ab_module_input)\n",
        "                    # bc_le_output = clf.le_module(bc_module_input)\n",
        "                    # ac_le_output = clf.le_module(ac_module_input)\n",
        "\n",
        "                    # diff = torch.abs(0.5 - ab_le_output) - torch.abs(0.5 - bc_le_output)\n",
        "                    # le_total_comp_loss += torch.sum( torch.sigmoid( (0.5 - ab_le_output) * 1e7 ) * torch.sigmoid( (bc_le_output - 0.5) * 1e7 ) * ( torch.sigmoid( ( torch.abs(0.5 - ab_le_output) - torch.abs(0.5 - bc_le_output) ) * 1e7 ) * torch.abs( ac_le_output - (0.5 - diff) ) + torch.sigmoid( ( torch.abs(0.5 - bc_le_output) - torch.abs(0.5 - ab_le_output) ) * 1e7 ) * torch.abs( (0.5 - diff) - ac_le_output ) ) )\n",
        "\n",
        "                    # diff = torch.abs(0.5 - bc_le_output) - torch.abs(0.5 - ab_le_output)\n",
        "                    # le_total_comp_loss += torch.sum( torch.sigmoid( (ab_le_output - 0.5) * 1e7 ) * torch.sigmoid( (0.5 - bc_le_output) * 1e7 ) * ( torch.sigmoid( ( torch.abs(0.5 - bc_le_output) - torch.abs(0.5 - ab_le_output) ) * 1e7 ) * torch.abs( ac_le_output - (0.5 - diff) ) + torch.sigmoid( ( torch.abs(0.5 - ab_le_output) - torch.abs(0.5 - bc_le_output) ) * 1e7 ) * torch.abs( (0.5 - diff) - ac_le_output ) ) )\n",
        "\n",
        "                    # ba_module_input = torch.cat( [feature_all_embed_2, feature_all_embed_1], axis = 1 )\n",
        "                    # cb_module_input = torch.cat( [feature_all_embed_3, feature_all_embed_2], axis = 1 )\n",
        "                    # ca_module_input = torch.cat( [feature_all_embed_3, feature_all_embed_1], axis = 1 )\n",
        "\n",
        "                    # ba_le_output = clf.le_module(ba_module_input)\n",
        "                    # cb_le_output = clf.le_module(cb_module_input)\n",
        "                    # ca_le_output = clf.le_module(ca_module_input)\n",
        "\n",
        "                    # diff = torch.abs(0.5 - ba_le_output) - torch.abs(0.5 - cb_le_output)\n",
        "                    # le_total_comp_loss += torch.sum( torch.sigmoid( (0.5 - ba_le_output) * 1e7 ) * torch.sigmoid( (cb_le_output - 0.5) * 1e7 ) * ( torch.sigmoid( ( torch.abs(0.5 - ba_le_output) - torch.abs(0.5 - cb_le_output) ) * 1e7 ) * torch.abs( ca_le_output - (0.5 - diff) ) + torch.sigmoid( ( torch.abs(0.5 - cb_le_output) - torch.abs(0.5 - ba_le_output) ) * 1e7 ) * torch.abs( (0.5 - diff) - ca_le_output ) ) )\n",
        "\n",
        "                    # diff = torch.abs(0.5 - cb_le_output) - torch.abs(0.5 - ba_le_output)\n",
        "                    # le_total_comp_loss += torch.sum( torch.sigmoid( (ba_le_output - 0.5) * 1e7 ) * torch.sigmoid( (0.5 - cb_le_output) * 1e7 ) * ( torch.sigmoid( ( torch.abs(0.5 - cb_le_output) - torch.abs(0.5 - ba_le_output) ) * 1e7 ) * torch.abs( ca_le_output - (0.5 - diff) ) + torch.sigmoid( ( torch.abs(0.5 - ba_le_output) - torch.abs(0.5 - cb_le_output) ) * 1e7 ) * torch.abs( (0.5 - diff) - ca_le_output ) ) )\n",
        "\n",
        "\n",
        "                # now compute ge nodes ref & asym & trans reg losses:\n",
        "                current_feature_bin_embed = clf.context_value_embeddings(ge_ref_lookup_indexes)\n",
        "                feature_bin_input = torch.cat( [current_feature_bin_embed, current_feature_bin_embed], axis = 1 )\n",
        "\n",
        "                ge_total_ref_loss += torch.sum( torch.abs( 0.5 - clf.ge_module(feature_bin_input) ) )\n",
        "\n",
        "\n",
        "                for i in range(num_of_numerical_predictors):\n",
        "                    feature_bin_embed = clf.num_feature_embeddings( ge_features_Si_indexes[i][0] )\n",
        "                    feature_context_embed = clf.context_value_embeddings( ge_features_Si_indexes[i][1] )\n",
        "\n",
        "                    feature_all_embed = torch.cat( [feature_bin_embed, feature_context_embed], axis = 0 )\n",
        "                    feature_all_embed_1 = feature_all_embed[ torch.randperm(feature_all_embed.size()[0]) ]\n",
        "                    feature_all_embed_2 = feature_all_embed[ torch.randperm(feature_all_embed.size()[0]) ]\n",
        "\n",
        "                    ab_module_input = torch.cat( [feature_all_embed_1, feature_all_embed_2], axis = 1 )\n",
        "                    ba_module_input = torch.cat( [feature_all_embed_2, feature_all_embed_1], axis = 1 )\n",
        "\n",
        "                    ge_total_asym_loss += torch.sum( torch.abs( clf.ge_module(ab_module_input) - ( 1.0 - clf.ge_module(ba_module_input) ) ) )\n",
        "\n",
        "\n",
        "                    feature_all_embed_1 = feature_all_embed[ torch.randperm(feature_all_embed.size()[0]) ]\n",
        "                    feature_all_embed_2 = feature_all_embed[ torch.randperm(feature_all_embed.size()[0]) ]\n",
        "                    feature_all_embed_3 = feature_all_embed[ torch.randperm(feature_all_embed.size()[0]) ]\n",
        "\n",
        "                    ab_module_input = torch.cat( [feature_all_embed_1, feature_all_embed_2], axis = 1 )\n",
        "                    bc_module_input = torch.cat( [feature_all_embed_2, feature_all_embed_3], axis = 1 )\n",
        "                    ac_module_input = torch.cat( [feature_all_embed_1, feature_all_embed_3], axis = 1 )\n",
        "\n",
        "                    ab_ge_output = clf.ge_module(ab_module_input)\n",
        "                    bc_ge_output = clf.ge_module(bc_module_input)\n",
        "                    ac_ge_output = clf.ge_module(ac_module_input)\n",
        "\n",
        "                    ge_total_trans_loss += torch.sum( torch.sigmoid( (ab_ge_output - 0.5) * 1e7 ) * torch.sigmoid( (bc_ge_output - 0.5) * 1e7 ) * relu( torch.max(torch.cat( [ ab_ge_output, bc_ge_output ], axis = 1 ), 1)[0].view(-1, 1) - ac_ge_output ) )\n",
        "                    ge_total_trans_loss += torch.sum( torch.sigmoid( (0.5 - ab_ge_output) * 1e7 ) * torch.sigmoid( (0.5 - bc_ge_output) * 1e7 ) * relu( ac_ge_output - torch.min(torch.cat( [ ab_ge_output, bc_ge_output ], axis = 1 ), 1)[0].view(-1, 1) ) )\n",
        "\n",
        "                    ba_module_input = torch.cat( [feature_all_embed_2, feature_all_embed_1], axis = 1 )\n",
        "                    cb_module_input = torch.cat( [feature_all_embed_3, feature_all_embed_2], axis = 1 )\n",
        "                    ca_module_input = torch.cat( [feature_all_embed_3, feature_all_embed_1], axis = 1 )\n",
        "\n",
        "                    ba_ge_output = clf.ge_module(ba_module_input)\n",
        "                    cb_ge_output = clf.ge_module(cb_module_input)\n",
        "                    ca_ge_output = clf.ge_module(ca_module_input)\n",
        "\n",
        "                    ge_total_trans_loss += torch.sum( torch.sigmoid( (ba_ge_output - 0.5) * 1e7 ) * torch.sigmoid( (cb_ge_output - 0.5) * 1e7 ) * relu( torch.max(torch.cat( [ ba_ge_output, cb_ge_output ], axis = 1 ), 1)[0].view(-1, 1) - ca_ge_output ) )\n",
        "                    ge_total_trans_loss += torch.sum( torch.sigmoid( (0.5 - ba_ge_output) * 1e7 ) * torch.sigmoid( (0.5 - cb_ge_output) * 1e7 ) * relu( ca_ge_output - torch.min(torch.cat( [ ba_ge_output, cb_ge_output ], axis = 1 ), 1)[0].view(-1, 1) ) )\n",
        "\n",
        "\n",
        "                    # feature_all_embed_1 = feature_all_embed[ torch.randperm(feature_all_embed.size()[0]) ]\n",
        "                    # feature_all_embed_2 = feature_all_embed[ torch.randperm(feature_all_embed.size()[0]) ]\n",
        "                    # feature_all_embed_3 = feature_all_embed[ torch.randperm(feature_all_embed.size()[0]) ]\n",
        "\n",
        "                    # ab_module_input = torch.cat( [feature_all_embed_1, feature_all_embed_2], axis = 1 )\n",
        "                    # bc_module_input = torch.cat( [feature_all_embed_2, feature_all_embed_3], axis = 1 )\n",
        "                    # ac_module_input = torch.cat( [feature_all_embed_1, feature_all_embed_3], axis = 1 )\n",
        "\n",
        "                    # ab_ge_output = clf.ge_module(ab_module_input)\n",
        "                    # bc_ge_output = clf.ge_module(bc_module_input)\n",
        "                    # ac_ge_output = clf.ge_module(ac_module_input)\n",
        "\n",
        "                    # diff = torch.abs(0.5 - ab_ge_output) - torch.abs(0.5 - bc_ge_output)\n",
        "                    # ge_total_comp_loss += torch.sum( torch.sigmoid( (0.5 - ab_ge_output) * 1e7 ) * torch.sigmoid( (bc_ge_output - 0.5) * 1e7 ) * ( torch.sigmoid( ( torch.abs(0.5 - ab_ge_output) - torch.abs(0.5 - bc_ge_output) ) * 1e7 ) * torch.abs( ac_ge_output - (0.5 - diff) ) + torch.sigmoid( ( torch.abs(0.5 - bc_ge_output) - torch.abs(0.5 - ab_ge_output) ) * 1e7 ) * torch.abs( (0.5 - diff) - ac_ge_output ) ) )\n",
        "\n",
        "                    # diff = torch.abs(0.5 - bc_ge_output) - torch.abs(0.5 - ab_ge_output)\n",
        "                    # ge_total_comp_loss += torch.sum( torch.sigmoid( (ab_ge_output - 0.5) * 1e7 ) * torch.sigmoid( (0.5 - bc_ge_output) * 1e7 ) * ( torch.sigmoid( ( torch.abs(0.5 - bc_ge_output) - torch.abs(0.5 - ab_ge_output) ) * 1e7 ) * torch.abs( ac_ge_output - (0.5 - diff) ) + torch.sigmoid( ( torch.abs(0.5 - ab_ge_output) - torch.abs(0.5 - bc_ge_output) ) * 1e7 ) * torch.abs( (0.5 - diff) - ac_ge_output ) ) )\n",
        "\n",
        "                    # ba_module_input = torch.cat( [feature_all_embed_2, feature_all_embed_1], axis = 1 )\n",
        "                    # cb_module_input = torch.cat( [feature_all_embed_3, feature_all_embed_2], axis = 1 )\n",
        "                    # ca_module_input = torch.cat( [feature_all_embed_3, feature_all_embed_1], axis = 1 )\n",
        "\n",
        "                    # ba_ge_output = clf.ge_module(ba_module_input)\n",
        "                    # cb_ge_output = clf.ge_module(cb_module_input)\n",
        "                    # ca_ge_output = clf.ge_module(ca_module_input)\n",
        "\n",
        "                    # diff = torch.abs(0.5 - ba_ge_output) - torch.abs(0.5 - cb_ge_output)\n",
        "                    # ge_total_comp_loss += torch.sum( torch.sigmoid( (0.5 - ba_ge_output) * 1e7 ) * torch.sigmoid( (cb_ge_output - 0.5) * 1e7 ) * ( torch.sigmoid( ( torch.abs(0.5 - ba_ge_output) - torch.abs(0.5 - cb_ge_output) ) * 1e7 ) * torch.abs( ca_ge_output - (0.5 - diff) ) + torch.sigmoid( ( torch.abs(0.5 - cb_ge_output) - torch.abs(0.5 - ba_ge_output) ) * 1e7 ) * torch.abs( (0.5 - diff) - ca_ge_output ) ) )\n",
        "\n",
        "                    # diff = torch.abs(0.5 - bc_ge_output) - torch.abs(0.5 - ab_ge_output)\n",
        "                    # ge_total_comp_loss += torch.sum( torch.sigmoid( (ba_ge_output - 0.5) * 1e7 ) * torch.sigmoid( (0.5 - cb_ge_output) * 1e7 ) * ( torch.sigmoid( ( torch.abs(0.5 - cb_ge_output) - torch.abs(0.5 - ba_ge_output) ) * 1e7 ) * torch.abs( ca_ge_output - (0.5 - diff) ) + torch.sigmoid( ( torch.abs(0.5 - ba_ge_output) - torch.abs(0.5 - cb_ge_output) ) * 1e7 ) * torch.abs( (0.5 - diff) - ca_ge_output ) ) )\n",
        "\n",
        "\n",
        "                # compute new le & ge reg loss w/ static bin diff.:\n",
        "                for i in range(bins):\n",
        "                    current_feature_bin_embed = clf.num_feature_embeddings( le_current_feature_indexes[i] )\n",
        "                    next_feature_bin_embed = clf.num_feature_embeddings( le_next_feature_indexes[i] )\n",
        "\n",
        "                    feature_bin_input_1 = torch.cat( [current_feature_bin_embed, next_feature_bin_embed], axis = 2 )\n",
        "                    feature_bin_input_2 = torch.cat( [next_feature_bin_embed, current_feature_bin_embed], axis = 2 )\n",
        "\n",
        "                    total_le_loss += torch.sum( torch.abs( le_expected_outputs_1[i] - torch.squeeze( clf.le_module(feature_bin_input_1) ) ) )\n",
        "                    total_le_loss += torch.sum( torch.abs( le_expected_outputs_2[i] - torch.squeeze( clf.le_module(feature_bin_input_2) ) ) )\n",
        "\n",
        "\n",
        "                    current_feature_bin_embed = clf.num_feature_embeddings( ge_current_feature_indexes[i] )\n",
        "                    next_feature_bin_embed = clf.num_feature_embeddings( ge_next_feature_indexes[i] )\n",
        "\n",
        "                    feature_bin_input_1 = torch.cat( [current_feature_bin_embed, next_feature_bin_embed], axis = 2 )\n",
        "                    feature_bin_input_2 = torch.cat( [next_feature_bin_embed, current_feature_bin_embed], axis = 2 )\n",
        "\n",
        "                    total_ge_loss += torch.sum( torch.abs( ge_expected_outputs_1[i] - torch.squeeze( clf.ge_module(feature_bin_input_1) ) ) )\n",
        "                    total_ge_loss += torch.sum( torch.abs( ge_expected_outputs_2[i] - torch.squeeze( clf.ge_module(feature_bin_input_2) ) ) )\n",
        "\n",
        "\n",
        "                # batch_reweight = reg_weight * reg_frequency * ( 1.0/ batch_size )\n",
        "                # loss = bce_loss(preds, target) + batch_reweight * (total_le_loss + total_ge_loss + le_total_ref_loss + le_total_asym_loss + le_total_trans_loss + 10.0 * le_total_ordering_loss + 10.0 * le_total_cutoff_loss + ge_total_ref_loss + ge_total_asym_loss + ge_total_trans_loss + 10.0 * ge_total_ordering_loss + 10.0 * ge_total_cutoff_loss) # + le_total_comp_loss + ge_total_comp_loss\n",
        "\n",
        "                le_batch_reweight = le_reg_weight * reg_frequency *  ( 1.0/ batch_size )\n",
        "                ge_batch_reweight = ge_reg_weight * reg_frequency *  ( 1.0/ batch_size )\n",
        "\n",
        "                loss = bce_loss(preds, target) + le_batch_reweight * (total_le_loss + le_total_ref_loss + le_total_asym_loss + le_total_trans_loss + 10.0 * le_total_ordering_loss + 10.0 * le_total_cutoff_loss) + ge_batch_reweight * (total_ge_loss + ge_total_ref_loss + ge_total_asym_loss + ge_total_trans_loss + 10.0 * ge_total_ordering_loss + 10.0 * ge_total_cutoff_loss)\n",
        "\n",
        "\n",
        "                if (epoch == 0 and reg_count == 0):\n",
        "                    init_num_regs_losses = (total_le_loss + total_ge_loss + le_total_ref_loss + le_total_asym_loss + le_total_trans_loss + 10.0 * le_total_ordering_loss + 10.0 * le_total_cutoff_loss + ge_total_ref_loss + ge_total_asym_loss + ge_total_trans_loss + 10.0 * ge_total_ordering_loss + 10.0 * ge_total_cutoff_loss) # + le_total_comp_loss + ge_total_comp_loss\n",
        "\n",
        "                if (epoch != 0):\n",
        "                    end_num_regs_losses = (total_le_loss + total_ge_loss + le_total_ref_loss + le_total_asym_loss + le_total_trans_loss + 10.0 * le_total_ordering_loss + 10.0 * le_total_cutoff_loss + ge_total_ref_loss + ge_total_asym_loss + ge_total_trans_loss + 10.0 * ge_total_ordering_loss + 10.0 * ge_total_cutoff_loss) # + le_total_comp_loss + ge_total_comp_loss\n",
        "\n",
        "                if reg_count == 0:\n",
        "                    print('total_le_loss: ', total_le_loss)\n",
        "                    print('total_ge_loss: ', total_ge_loss)\n",
        "                    print('le_total_ref_loss: ', le_total_ref_loss)\n",
        "                    print('le_total_asym_loss: ', le_total_asym_loss)\n",
        "                    print('le_total_trans_loss: ', le_total_trans_loss)\n",
        "                    print('le_total_ordering_loss: ', le_total_ordering_loss)\n",
        "                    print('le_total_cutoff_loss: ', le_total_cutoff_loss)\n",
        "                    print('le_total_comp_loss: ', le_total_comp_loss)\n",
        "                    print('ge_total_ref_loss: ', ge_total_ref_loss)\n",
        "                    print('ge_total_asym_loss: ', ge_total_asym_loss)\n",
        "                    print('ge_total_trans_loss: ', ge_total_trans_loss)\n",
        "                    print('ge_total_ordering_loss: ', ge_total_ordering_loss)\n",
        "                    print('ge_total_cutoff_loss: ', ge_total_cutoff_loss)\n",
        "                    print('ge_total_comp_loss: ', ge_total_comp_loss)\n",
        "\n",
        "            else:\n",
        "                loss = bce_loss(preds, target)\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            clf.zero_grad()\n",
        "\n",
        "            total_loss += loss\n",
        "            batch_count += 1\n",
        "            reg_count += 1\n",
        "\n",
        "\n",
        "        # perform early-stopping:\n",
        "        clf.NSDT_masking = False\n",
        "        clf = clf.eval()\n",
        "\n",
        "        # Create context value index lists for training set:\n",
        "        val_all_batch_le_context_value_index_list = []\n",
        "        val_all_batch_ge_context_value_index_list = []\n",
        "        val_all_batch_be_context_value_index_list = []\n",
        "\n",
        "        for batch in val_loader:\n",
        "\n",
        "            val_batch_size = len(batch)\n",
        "            val_batch_le_context_value_index_list = []\n",
        "            val_batch_ge_context_value_index_list = []\n",
        "            val_batch_be_context_value_index_list = []\n",
        "\n",
        "            for node in le_nodes:\n",
        "                le_context_value_index_list = [ node[2] ] * val_batch_size\n",
        "                val_batch_le_context_value_index_list.append(le_context_value_index_list)\n",
        "\n",
        "            for node in ge_nodes:\n",
        "                ge_context_value_index_list = [ node[2] ] * val_batch_size\n",
        "                val_batch_ge_context_value_index_list.append(ge_context_value_index_list)\n",
        "\n",
        "            for node in be_nodes:\n",
        "                be_context_value_index_list = [ node[2] ] * val_batch_size\n",
        "                val_batch_be_context_value_index_list.append(be_context_value_index_list)\n",
        "\n",
        "            val_all_batch_le_context_value_index_list.append(val_batch_le_context_value_index_list)\n",
        "            val_all_batch_ge_context_value_index_list.append(val_batch_ge_context_value_index_list)\n",
        "            val_all_batch_be_context_value_index_list.append(val_batch_be_context_value_index_list)\n",
        "\n",
        "\n",
        "        val_pred_labels = []\n",
        "\n",
        "        batch_len = 0\n",
        "\n",
        "        val_feature_indexes_le_nodes  = []\n",
        "        val_feature_indexes_ge_nodes  = []\n",
        "        val_feature_indexes_be_nodes  = []\n",
        "\n",
        "        for node in le_nodes:\n",
        "            val_feature_indexes_le_nodes.append(node[0] - 1)\n",
        "\n",
        "        for node in ge_nodes:\n",
        "            val_feature_indexes_ge_nodes.append(node[0] - 1)\n",
        "\n",
        "        for node in be_nodes:\n",
        "            val_feature_indexes_be_nodes.append(node[0] - 1)\n",
        "\n",
        "\n",
        "        for batch in val_loader:\n",
        "\n",
        "            subset = batch[:, :] # X only; target already removed\n",
        "            val_batch_size = len(batch)\n",
        "\n",
        "            val_batch_le_feature_index_list = subset[:, val_feature_indexes_le_nodes].permute(1,0).to('cuda')\n",
        "            val_batch_ge_feature_index_list = subset[:, val_feature_indexes_ge_nodes].permute(1,0).to('cuda')\n",
        "\n",
        "            val_batch_le_context_value_index_list = val_all_batch_le_context_value_index_list[batch_len]\n",
        "            val_batch_ge_context_value_index_list = val_all_batch_ge_context_value_index_list[batch_len]\n",
        "\n",
        "            val_preds = clf.forward(val_batch_le_feature_index_list, val_batch_le_context_value_index_list,\n",
        "                                val_batch_ge_feature_index_list, val_batch_ge_context_value_index_list,\n",
        "                                val_batch_size )\n",
        "\n",
        "\n",
        "            val_preds = torch.sigmoid(val_preds)\n",
        "            val_preds = torch.squeeze(val_preds).tolist()\n",
        "\n",
        "            batch_len += 1\n",
        "\n",
        "            for i in val_preds:\n",
        "                if i > 0.5:\n",
        "                    val_pred_labels.append(1)\n",
        "                if i <= 0.5:\n",
        "                    val_pred_labels.append(0)\n",
        "\n",
        "        # Compute Balanced Accuracy:\n",
        "        val_accuracy = metrics.balanced_accuracy_score(val_data['target'], val_pred_labels)\n",
        "\n",
        "\n",
        "        # now compute test accuracy for bayes optimization record\n",
        "        # Create context value index lists for training set:\n",
        "        test_all_batch_le_context_value_index_list = []\n",
        "        test_all_batch_ge_context_value_index_list = []\n",
        "        test_all_batch_be_context_value_index_list = []\n",
        "\n",
        "        for batch in test_loader:\n",
        "\n",
        "            test_batch_size = len(batch)\n",
        "            test_batch_le_context_value_index_list = []\n",
        "            test_batch_ge_context_value_index_list = []\n",
        "            test_batch_be_context_value_index_list = []\n",
        "\n",
        "            for node in le_nodes:\n",
        "                le_context_value_index_list = [ node[2] ] * test_batch_size\n",
        "                test_batch_le_context_value_index_list.append(le_context_value_index_list)\n",
        "\n",
        "            for node in ge_nodes:\n",
        "                ge_context_value_index_list = [ node[2] ] * test_batch_size\n",
        "                test_batch_ge_context_value_index_list.append(ge_context_value_index_list)\n",
        "\n",
        "            for node in be_nodes:\n",
        "                be_context_value_index_list = [ node[2] ] * test_batch_size\n",
        "                test_batch_be_context_value_index_list.append(be_context_value_index_list)\n",
        "\n",
        "            test_all_batch_le_context_value_index_list.append(test_batch_le_context_value_index_list)\n",
        "            test_all_batch_ge_context_value_index_list.append(test_batch_ge_context_value_index_list)\n",
        "            test_all_batch_be_context_value_index_list.append(test_batch_be_context_value_index_list)\n",
        "\n",
        "\n",
        "        test_pred_labels = []\n",
        "\n",
        "        batch_len = 0\n",
        "\n",
        "        test_feature_indexes_le_nodes  = []\n",
        "        test_feature_indexes_ge_nodes  = []\n",
        "        test_feature_indexes_be_nodes  = []\n",
        "\n",
        "        for node in le_nodes:\n",
        "            test_feature_indexes_le_nodes.append(node[0] - 1)\n",
        "\n",
        "        for node in ge_nodes:\n",
        "            test_feature_indexes_ge_nodes.append(node[0] - 1)\n",
        "\n",
        "        for node in be_nodes:\n",
        "            test_feature_indexes_be_nodes.append(node[0] - 1)\n",
        "\n",
        "\n",
        "        for batch in test_loader:\n",
        "\n",
        "            subset = batch[:, :] # X only; target already removed\n",
        "            test_batch_size = len(batch)\n",
        "\n",
        "            test_batch_le_feature_index_list = subset[:, test_feature_indexes_le_nodes].permute(1,0).to('cuda')\n",
        "            test_batch_ge_feature_index_list = subset[:, test_feature_indexes_ge_nodes].permute(1,0).to('cuda')\n",
        "\n",
        "            test_batch_le_context_value_index_list = test_all_batch_le_context_value_index_list[batch_len]\n",
        "            test_batch_ge_context_value_index_list = test_all_batch_ge_context_value_index_list[batch_len]\n",
        "\n",
        "            test_preds = clf.forward(test_batch_le_feature_index_list, test_batch_le_context_value_index_list,\n",
        "                                test_batch_ge_feature_index_list, test_batch_ge_context_value_index_list,\n",
        "                                test_batch_size )\n",
        "\n",
        "\n",
        "            test_preds = torch.sigmoid(test_preds)\n",
        "            test_preds = torch.squeeze(test_preds).tolist()\n",
        "\n",
        "            batch_len += 1\n",
        "\n",
        "            for i in test_preds:\n",
        "                if i > 0.5:\n",
        "                    test_pred_labels.append(1)\n",
        "                if i <= 0.5:\n",
        "                    test_pred_labels.append(0)\n",
        "\n",
        "        # Compute Balanced Accuracy:\n",
        "        test_accuracy = metrics.balanced_accuracy_score(test_data['target'], test_pred_labels)\n",
        "\n",
        "        clf = clf.train()\n",
        "        clf.NSDT_masking = masking\n",
        "\n",
        "        accs.append( [val_accuracy, test_accuracy] )\n",
        "\n",
        "        if early_stopper.early_stop(-val_accuracy):\n",
        "            if dataset == 'Higgs':\n",
        "                return clf, accs[ len(accs) - early_stopper.patience - 1 ][0], accs[ len(accs) - early_stopper.patience - 1 ][1], init_num_regs_losses, end_num_regs_losses, init_cat_regs_losses, end_cat_regs_losses\n",
        "            else:\n",
        "                return clf, accs[ len(accs) - 1 ][0], accs[ len(accs) - 1 ][1], init_num_regs_losses, end_num_regs_losses, init_cat_regs_losses, end_cat_regs_losses\n",
        "\n",
        "        print()\n",
        "        print( epoch + 1, ': ', 'total loss: ', total_loss, '; val accuracy: ', accs[epoch][0] )\n",
        "\n",
        "\n",
        "    return clf, accs[ len(accs) - 1 ][0], accs[ len(accs) - 1 ][1], init_num_regs_losses, end_num_regs_losses\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "UdGSGvq9hLvk"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "zLCXx6qdxqoh"
      },
      "outputs": [],
      "source": [
        "if (dataset == 'Higgs'):\n",
        "\n",
        "    # Load dataset & change target feature name to target:\n",
        "    data = pd.read_csv('higgs.csv', low_memory = False)\n",
        "    data = data.rename( columns={'class': 'target'} )\n",
        "\n",
        "    # replace target feature at the end of features\n",
        "    headers = list(data.columns)\n",
        "    headers.remove('target')\n",
        "    headers.append('target')\n",
        "    data = data[headers]\n",
        "\n",
        "    # drop samples with missing cells (only 8 in total)\n",
        "    data = data.replace('?', np.nan)\n",
        "    data.dropna(inplace = True)\n",
        "\n",
        "    # Treat jet1b-tag to jet4b-tag features (4 in total) as Ordinal Categorical\n",
        "    for i in data.columns:\n",
        "        if (i != 'target'):\n",
        "            print(i, data[i].dtype)\n",
        "            if (data[i].nunique() == 3):\n",
        "                data[i] = data[i].astype('object')\n",
        "            else:\n",
        "                data[i] = data[i].astype('float')\n",
        "\n",
        "    # place all categorical features after all numerical features\n",
        "    nums = []\n",
        "    cats = []\n",
        "    for i in data.columns:\n",
        "        if (i != 'target'):\n",
        "            if pd.api.types.is_numeric_dtype(data[i]):\n",
        "                nums.append(i)\n",
        "            else:\n",
        "                cats.append(i)\n",
        "\n",
        "    headers = []\n",
        "    for i in nums:\n",
        "        headers.append(i)\n",
        "    for i in cats:\n",
        "        headers.append(i)\n",
        "    headers.append('target')\n",
        "    data = data[headers]\n",
        "\n",
        "    print(data)\n",
        "    print()\n",
        "\n",
        "    # Change all feature names to Feature_1 - Feature_N\n",
        "    original_features = []\n",
        "    predictors = list(data.columns)\n",
        "    headers = []\n",
        "    for i in range( 1, len(data.columns) ):\n",
        "        s = 'Feature' + str(i)\n",
        "        original_features.append( predictors[i - 1] )\n",
        "        headers.append(s)\n",
        "    headers.append('target')\n",
        "    data.columns = headers\n",
        "\n",
        "    print(data)\n",
        "    print()\n",
        "\n",
        "    for i in original_features:\n",
        "        print(i)\n",
        "    print()\n",
        "\n",
        "    # Peform stratified train-test split based on a random seed\n",
        "    predictors = list(data.columns)\n",
        "    predictors.remove('target')\n",
        "    X_train, X_test, Y_train, Y_test = train_test_split(data[predictors], data['target'], test_size = 0.2, random_state = 1, stratify = data['target'])\n",
        "\n",
        "    train_data = X_train\n",
        "    train_data['target'] = Y_train\n",
        "\n",
        "    test_data = X_test\n",
        "    test_data['target'] = Y_test\n",
        "\n",
        "    X_train, X_val, Y_train, Y_val = train_test_split(train_data[predictors], train_data['target'], test_size = 0.125, random_state = 1, stratify = train_data['target'])\n",
        "\n",
        "    train_data = X_train\n",
        "    train_data['target'] = Y_train\n",
        "\n",
        "    val_data = X_val\n",
        "    val_data['target'] = Y_val\n",
        "\n",
        "    print(val_data)\n",
        "    print()\n",
        "    print(test_data)\n",
        "\n",
        "    training_reweight = 1.0 # well-balanced between 2 classes; no need to reweight\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "s5ENwOr0xq4l"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "P7Fw-I0_FvbC"
      },
      "outputs": [],
      "source": [
        "# Drop feature 25 as it's sample weight;\n",
        "def Census_processing(train_data):\n",
        "    headers = []\n",
        "    for i in range(1,42):\n",
        "        if (i < 25):\n",
        "            s = 'Feature' + str(i)\n",
        "        elif (i == 25):\n",
        "            s = 'Feature_to_drop'\n",
        "        else:\n",
        "            s = 'Feature' + str(i - 1)\n",
        "\n",
        "        headers.append(s)\n",
        "\n",
        "    headers.append('target')\n",
        "    train_data.columns = headers\n",
        "\n",
        "    cont_features = ['Feature1', 'Feature6', 'Feature17', 'Feature18', 'Feature19', 'Feature30', 'Feature39']\n",
        "    for i in train_data.columns:\n",
        "        if (i != 'target'):\n",
        "            if i in cont_features:\n",
        "                train_data[i] = train_data[i].astype('float')\n",
        "            else:\n",
        "                train_data[i] = train_data[i].astype('object')\n",
        "\n",
        "    train_data['target'] = train_data['target'].replace( [' - 50000.', ' 50000+.'], [0, 1] )\n",
        "    train_data = train_data.drop( columns = ['Feature_to_drop'])\n",
        "\n",
        "    oridinal_feature_names = ['age', 'class of worker', 'detailed industry recode', 'detailed occupation recode', 'education', 'wage per hour', 'enroll in edu inst last wk',\n",
        "    'marital status', 'major industry code', 'major occupation code', 'race', 'hispanic origin', 'sex', 'member of a labor union', 'reason for unemployment',\n",
        "    'full or part time employment status', 'capital gains', 'capital losses', 'dividends from stocks', 'tax filer status', 'region of previous residence',\n",
        "    'state of previous residence', 'detailed household and family status', 'detailed household summary in household', 'migration code-change in msa',\n",
        "    'migration code-change in reg', 'migration code-move within reg', 'live in this house 1 year ago', 'migration prev res in sunbelt', 'num persons worked for employer',\n",
        "    'family members under 18', 'country of birth father', 'country of birth mother', 'country of birth self', 'citizenship', 'own business or self employed',\n",
        "    'fill inc questionnaire for veterans admin', 'veterans benefits', 'weeks worked in year', 'year', 'target']\n",
        "    train_data.columns = oridinal_feature_names\n",
        "\n",
        "    dup_header = list(train_data.columns)\n",
        "    dup_header.remove('target')\n",
        "\n",
        "    print( len(train_data) )\n",
        "    train_data = train_data.drop_duplicates(subset = dup_header)\n",
        "    print( len(train_data) )\n",
        "    print()\n",
        "\n",
        "    # place all categorical features after all numerical features\n",
        "    nums = []\n",
        "    cats = []\n",
        "    for i in train_data.columns:\n",
        "        if (i != 'target'):\n",
        "            if pd.api.types.is_numeric_dtype(train_data[i]):\n",
        "                nums.append(i)\n",
        "            else:\n",
        "                cats.append(i)\n",
        "\n",
        "    cols = []\n",
        "    for i in nums:\n",
        "        cols.append(i)\n",
        "    for i in cats:\n",
        "        cols.append(i)\n",
        "    cols.append('target')\n",
        "    train_data = train_data[cols]\n",
        "\n",
        "    for column in cats:\n",
        "        train_data = train_data[ train_data[column] != ' ?' ]\n",
        "    print( len(train_data) )\n",
        "\n",
        "    # Change all feature names to Feature_1 - Feature_N\n",
        "    cols = []\n",
        "    original_features = []\n",
        "    predictors = list(train_data.columns)\n",
        "    for i in range( 1, len(train_data.columns) ):\n",
        "        original_features.append( predictors[i - 1] )\n",
        "        s = 'Feature' + str(i)\n",
        "        cols.append(s)\n",
        "    cols.append('target')\n",
        "    train_data.columns = cols\n",
        "\n",
        "    for i in train_data.columns:\n",
        "        print(i, train_data[i].dtype, train_data[i].nunique(), train_data[i].isna().sum()  )\n",
        "    print()\n",
        "\n",
        "    return train_data, original_features\n",
        "\n",
        "\n",
        "if (dataset == 'Census'):\n",
        "    train_data = pd.read_csv('census-income.csv')\n",
        "    train_data, original_features = Census_processing(train_data)\n",
        "\n",
        "    predictors = list(train_data.columns)\n",
        "    predictors.remove('target')\n",
        "    X_train, X_val, Y_train, Y_val = train_test_split(train_data[predictors], train_data['target'], test_size = 0.1, random_state = 0, stratify = train_data['target'])\n",
        "\n",
        "    # train_data = X_train\n",
        "    # train_data['target'] = Y_train\n",
        "\n",
        "    # Try both try both re-sampling & cost-sensitive learning\n",
        "    # https://imbalanced-learn.org/stable/references/generated/imblearn.under_sampling.RandomUnderSampler.html#imblearn.under_sampling.RandomUnderSampler\n",
        "    ros = RandomOverSampler()\n",
        "    X_ros, y_ros = ros.fit_resample(X_train, Y_train)\n",
        "    train_data = X_ros\n",
        "    train_data['target'] = y_ros\n",
        "\n",
        "    val_data = X_val\n",
        "    val_data['target'] = Y_val\n",
        "\n",
        "    print(len(val_data))\n",
        "    for i in train_data.columns:\n",
        "        val_extra_levels = list( set( list( val_data[i].unique() ) ) - set( list( train_data[i].unique() ) ) )\n",
        "        if len(val_extra_levels) > 0:\n",
        "            if (train_data[i].dtype == 'string' or train_data[i].dtype == 'object'):\n",
        "                print( i, ' ', len(val_extra_levels), ' ', val_extra_levels )\n",
        "\n",
        "                for level in val_extra_levels:\n",
        "                    print( len( val_data[ val_data[i] == level ] ) )\n",
        "                    val_data = val_data[val_data[i] != level]\n",
        "\n",
        "    print(len(val_data))\n",
        "    print()\n",
        "\n",
        "    test_data = pd.read_csv('census-income_test.csv')\n",
        "    test_data, original_features = Census_processing(test_data)\n",
        "\n",
        "    print(len(test_data))\n",
        "    for i in train_data.columns:\n",
        "        test_extra_levels = list( set( list( test_data[i].unique() ) ) - set( list( train_data[i].unique() ) ) )\n",
        "        if len(test_extra_levels) > 0:\n",
        "            if (train_data[i].dtype == 'string' or train_data[i].dtype == 'object'):\n",
        "                print( i, ' ', len(test_extra_levels), ' ', test_extra_levels )\n",
        "\n",
        "                for level in test_extra_levels:\n",
        "                    print( len( test_data[ test_data[i] == level ] ) )\n",
        "                    test_data = test_data[test_data[i] != level]\n",
        "\n",
        "    print(len(test_data))\n",
        "    print()\n",
        "\n",
        "    for i in original_features:\n",
        "        print(i)\n",
        "\n",
        "    class1_count = len( train_data[ train_data['target'] == 0 ] )\n",
        "    class2_count = len( train_data[ train_data['target'] == 1 ] )\n",
        "    print(class1_count / class2_count)\n",
        "\n",
        "    training_reweight = class1_count / class2_count\n",
        "\n",
        "    class1_count = len( test_data[ test_data['target'] == 0 ] )\n",
        "    class2_count = len( test_data[ test_data['target'] == 1 ] )\n",
        "    print(class1_count / class2_count)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "EdYmX2nwFvsO"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nn8Xt0Zn2be3"
      },
      "outputs": [],
      "source": [
        "if (dataset == 'Credit'):\n",
        "\n",
        "    data = pd.read_csv('credit_new.csv')\n",
        "    data = data.drop( columns = ['Unnamed: 0'])\n",
        "    data = data.rename( columns={'SeriousDlqin2yrs': 'target'} )\n",
        "\n",
        "    cols = list(data.columns)\n",
        "    cols.remove('target')\n",
        "    cols.append('target')\n",
        "    data = data[cols]\n",
        "    data.dropna(inplace = True)\n",
        "\n",
        "    features = list(data.columns)\n",
        "\n",
        "    headers = []\n",
        "    original_features = []\n",
        "    predictors = list(data.columns)\n",
        "    for i in range( 1, len(data.columns) ):\n",
        "        original_features.append( predictors[i - 1] )\n",
        "        s = 'Feature' + str(i)\n",
        "        headers.append(s)\n",
        "        print(i, ' ', features[i-1])\n",
        "    print()\n",
        "\n",
        "    headers.append('target')\n",
        "    data.columns = headers\n",
        "\n",
        "    for i in original_features:\n",
        "        print(i)\n",
        "\n",
        "    # Peform stratified train-test split based on a random seed\n",
        "    predictors = list(data.columns)\n",
        "    predictors.remove('target')\n",
        "    X_train, X_test, Y_train, Y_test = train_test_split(data[predictors], data['target'], test_size = 0.2, random_state = 0, stratify = data['target'])\n",
        "\n",
        "    train_data = X_train\n",
        "    train_data['target'] = Y_train\n",
        "\n",
        "    test_data = X_test\n",
        "    test_data['target'] = Y_test\n",
        "\n",
        "    X_train, X_val, Y_train, Y_val = train_test_split(train_data[predictors], train_data['target'], test_size = 0.125, random_state = 0, stratify = train_data['target'])\n",
        "\n",
        "    # Try both try both re-sampling & cost-sensitive learning\n",
        "    # train_data = X_train\n",
        "    # train_data['target'] = Y_train\n",
        "\n",
        "    ros = RandomOverSampler()\n",
        "    X_ros, y_ros = ros.fit_resample(X_train, Y_train)\n",
        "    train_data = X_ros\n",
        "    train_data['target'] = y_ros\n",
        "\n",
        "    val_data = X_val\n",
        "    val_data['target'] = Y_val\n",
        "\n",
        "    class1_count = len( train_data[ train_data['target'] == 0 ] )\n",
        "    class2_count = len( train_data[ train_data['target'] == 1 ] )\n",
        "    print(class1_count / class2_count)\n",
        "\n",
        "    training_reweight = class1_count / class2_count # increase reg loss weight to set minority weight higher\n",
        "    print(training_reweight)\n",
        "\n",
        "    class1_count = len( test_data[ test_data['target'] == 0 ] )\n",
        "    class2_count = len( test_data[ test_data['target'] == 1 ] )\n",
        "    print(class1_count / class2_count)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "eVt7wHKJ2cB3"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "if (dataset == 'Cover'):\n",
        "\n",
        "    data = pd.read_csv('covtype.csv')\n",
        "    data = data.rename( columns={'Cover_Type': 'target'} )\n",
        "\n",
        "    # convert the 2 most common types to 0, with the rest types (> 15%) being 1\n",
        "    data.loc[data['target'] == 1, 'target'] = 0\n",
        "    data.loc[data['target'] == 2, 'target'] = 0\n",
        "    data.loc[data['target'] != 0, 'target'] = 1\n",
        "\n",
        "    data = pd.read_csv('covtype.csv')\n",
        "    data = data.rename( columns={'Cover_Type': 'target'} )\n",
        "\n",
        "    data.loc[data['target'] == 1, 'target'] = 0\n",
        "    data.loc[data['target'] == 2, 'target'] = 0\n",
        "    data.loc[data['target'] != 0, 'target'] = 1\n",
        "\n",
        "    print( data['target'].unique() )\n",
        "    print(len(data[data['target'] == 0]))\n",
        "    print(len(data[data['target'] == 1]))\n",
        "    print(len(data[data['target'] == 0]) / len(data[data['target'] == 1]))\n",
        "\n",
        "    columns = data.columns\n",
        "\n",
        "    # Select column names from the 11th to the second-to-last column\n",
        "    selected_columns = columns[10:-1]\n",
        "\n",
        "    # Convert the selected columns to string type as they encode the presence or absence of a cateogrical feature\n",
        "    data[selected_columns] = data[selected_columns].astype(str)\n",
        "\n",
        "    cols = list(data.columns)\n",
        "    cols.remove('target')\n",
        "    cols.append('target')\n",
        "    data = data[cols]\n",
        "    data.dropna(inplace = True)\n",
        "\n",
        "    features = list(data.columns)\n",
        "\n",
        "    headers = []\n",
        "    original_features = []\n",
        "    predictors = list(data.columns)\n",
        "    for i in range( 1, len(data.columns) ):\n",
        "        original_features.append( predictors[i - 1] )\n",
        "        s = 'Feature' + str(i)\n",
        "        headers.append(s)\n",
        "        print(i, ' ', features[i-1])\n",
        "    print()\n",
        "\n",
        "    headers.append('target')\n",
        "    data.columns = headers\n",
        "\n",
        "    for i in original_features:\n",
        "        print(i)\n",
        "\n",
        "    # Peform stratified train-test split based on a random seed\n",
        "    predictors = list(data.columns)\n",
        "    predictors.remove('target')\n",
        "    X_train, X_test, Y_train, Y_test = train_test_split(data[predictors], data['target'], test_size = 0.2, random_state = 1, stratify = data['target'])\n",
        "\n",
        "    train_data = X_train\n",
        "    train_data['target'] = Y_train\n",
        "\n",
        "    test_data = X_test\n",
        "    test_data['target'] = Y_test\n",
        "\n",
        "    X_train, X_val, Y_train, Y_val = train_test_split(train_data[predictors], train_data['target'], test_size = 0.125, random_state = 1, stratify = train_data['target'])\n",
        "\n",
        "    train_data = X_train\n",
        "    train_data['target'] = Y_train\n",
        "\n",
        "    val_data = X_val\n",
        "    val_data['target'] = Y_val\n",
        "\n",
        "    print(val_data)\n",
        "    print()\n",
        "    print(test_data)\n",
        "\n",
        "    class1_count = len( train_data[ train_data['target'] == 0 ] )\n",
        "    class2_count = len( train_data[ train_data['target'] == 1 ] )\n",
        "    print(class1_count / class2_count)\n",
        "\n",
        "    training_reweight = class1_count / class2_count # increase reg loss weight to set minority weight higher\n"
      ],
      "metadata": {
        "id": "feQGaiAxNhZB"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "E-fBKn85Nhpg"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "Kxy8KJhPMkyK"
      },
      "outputs": [],
      "source": [
        "if (dataset == 'Insurance'):\n",
        "\n",
        "    data = pd.read_csv('all_state.csv')\n",
        "    data = data.rename( columns={'loss': 'target'} )\n",
        "    data = data.drop( columns = ['id'] )\n",
        "\n",
        "    # convert target s/t the top 10% will be 1 (most severe ones), while others being 0;\n",
        "    threshold = data['target'].quantile(.9)\n",
        "    print( threshold, data['target'].max() )\n",
        "\n",
        "    data.loc[data['target'] < threshold, 'target'] = 0\n",
        "    data.loc[data['target'] >= threshold, 'target'] = 1\n",
        "    data['target'] = data['target'].astype('int')\n",
        "    print( data['target'].dtype )\n",
        "    print()\n",
        "\n",
        "    # place all categorical features after all numerical features\n",
        "    nums = []\n",
        "    cats = []\n",
        "    for i in data.columns:\n",
        "        if (i != 'target'):\n",
        "            if pd.api.types.is_numeric_dtype(data[i]):\n",
        "                nums.append(i)\n",
        "            else:\n",
        "                cats.append(i)\n",
        "\n",
        "    headers = []\n",
        "    for i in nums:\n",
        "        headers.append(i)\n",
        "    for i in cats:\n",
        "        headers.append(i)\n",
        "    headers.append('target')\n",
        "    data = data[headers]\n",
        "\n",
        "    # Change all feature names to Feature_1 - Feature_N\n",
        "    headers = []\n",
        "    original_features = []\n",
        "    predictors = list(data.columns)\n",
        "    for i in range( 1, len(data.columns) ):\n",
        "        original_features.append( predictors[i - 1] )\n",
        "        s = 'Feature' + str(i)\n",
        "        headers.append(s)\n",
        "    headers.append('target')\n",
        "    data.columns = headers\n",
        "\n",
        "    for i in data.columns:\n",
        "        print( i, data[i].dtype, data[i].nunique(), data[i].isna().sum() )\n",
        "    print()\n",
        "\n",
        "    # Peform stratified train-test split based on a random seed\n",
        "    predictors = list(data.columns)\n",
        "    predictors.remove('target')\n",
        "    X_train, X_test, Y_train, Y_test = train_test_split(data[predictors], data['target'], test_size = 0.2, random_state = 0, stratify = data['target'])\n",
        "\n",
        "    train_data = X_train\n",
        "    train_data['target'] = Y_train\n",
        "\n",
        "    test_data = X_test\n",
        "    test_data['target'] = Y_test\n",
        "\n",
        "    X_train, X_val, Y_train, Y_val = train_test_split(train_data[predictors], train_data['target'], test_size = 0.125, random_state = 0, stratify = train_data['target'])\n",
        "\n",
        "    # Try both try both re-sampling & cost-sensitive learning\n",
        "    # train_data = X_train\n",
        "    # train_data['target'] = Y_train\n",
        "\n",
        "    ros = RandomOverSampler()\n",
        "    X_ros, y_ros = ros.fit_resample(X_train, Y_train)\n",
        "    train_data = X_ros\n",
        "    train_data['target'] = y_ros\n",
        "\n",
        "    val_data = X_val\n",
        "    val_data['target'] = Y_val\n",
        "\n",
        "    print(len(val_data))\n",
        "    for i in train_data.columns:\n",
        "        val_extra_levels = list( set( list( val_data[i].unique() ) ) - set( list( train_data[i].unique() ) ) )\n",
        "        if len(val_extra_levels) > 0:\n",
        "            if (train_data[i].dtype == 'string' or train_data[i].dtype == 'object'):\n",
        "                print( i, ' ', len(val_extra_levels), ' ', val_extra_levels )\n",
        "\n",
        "                for level in val_extra_levels:\n",
        "                    print( len( val_data[ val_data[i] == level ] ) )\n",
        "                    val_data = val_data[val_data[i] != level]\n",
        "\n",
        "    print(len(val_data))\n",
        "    print()\n",
        "\n",
        "    print(len(test_data))\n",
        "    for i in train_data.columns:\n",
        "        test_extra_levels = list( set( list( test_data[i].unique() ) ) - set( list( train_data[i].unique() ) ) )\n",
        "        if len(test_extra_levels) > 0:\n",
        "            if (train_data[i].dtype == 'string' or train_data[i].dtype == 'object'):\n",
        "                print( i, ' ', len(test_extra_levels), ' ', test_extra_levels )\n",
        "\n",
        "                for level in test_extra_levels:\n",
        "                    print( len( test_data[ test_data[i] == level ] ) )\n",
        "                    test_data = test_data[test_data[i] != level]\n",
        "\n",
        "    print(len(test_data))\n",
        "    print()\n",
        "\n",
        "    for i in original_features:\n",
        "        print(i)\n",
        "\n",
        "    class1_count = len( train_data[ train_data['target'] == 0 ] )\n",
        "    class2_count = len( train_data[ train_data['target'] == 1 ] )\n",
        "    print(class1_count / class2_count)\n",
        "\n",
        "    training_reweight = class1_count / class2_count # increase reg loss weight to set minority weight higher\n",
        "    print(training_reweight)\n",
        "\n",
        "    class1_count = len( test_data[ test_data['target'] == 0 ] )\n",
        "    class2_count = len( test_data[ test_data['target'] == 1 ] )\n",
        "    print(class1_count / class2_count)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D9nttO4IlPTr"
      },
      "source": [
        "# Model Training:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OQfwLHSoxbU7"
      },
      "outputs": [],
      "source": [
        "# Compute DTree-based paths/rules to be adapted:\n",
        "\n",
        "# make 2 copies of train sets for later use;\n",
        "train_copy = train_data.copy()\n",
        "train_copy_2 = train_data.copy()\n",
        "\n",
        "# pre-process training copy to fit with sk-learn decision tree:\n",
        "if (non_ordinal_cat == False):\n",
        "    # for ordinal Cat feature, simply convert to float\n",
        "    for i in train_copy.columns:\n",
        "        if (i != 'target'):\n",
        "            train_copy[i] = train_copy[i].astype('float64')\n",
        "\n",
        "else:\n",
        "    # for non-ordinal Cat feature, perform target encoding\n",
        "    for i in train_copy.columns:\n",
        "        if (i != 'target'):\n",
        "            if pd.api.types.is_numeric_dtype(train_copy[i]) == False:\n",
        "                print(i)\n",
        "                encoder = TargetEncoder()\n",
        "                train_copy[i] = encoder.fit_transform( train_copy[i], train_copy['target'] )\n",
        "\n",
        "\n",
        "# compute & show performance for DTree on processed data\n",
        "X_train = train_copy.iloc[ :, :len(train_copy.columns) - 1 ]\n",
        "Y_train = list( train_copy['target'] )\n",
        "\n",
        "clf = DecisionTreeClassifier(max_depth = tree_depth, class_weight = 'balanced', random_state = 1)\n",
        "clf.fit(X_train, Y_train)\n",
        "\n",
        "tree_rules = export_text(clf, feature_names = list(X_train.columns) )\n",
        "print(tree_rules)\n",
        "print()\n",
        "print()\n",
        "\n",
        "# get all features used during tree generation\n",
        "feature_set = []\n",
        "rules = get_rules(clf, list(X_train.columns), Y_train)\n",
        "for i in range( len(rules) ):\n",
        "    for j in range( len(rules[i]) ):\n",
        "        rules[i][j] = rules[i][j].split(' ')\n",
        "        feature_set.append( rules[i][j][0] )\n",
        "\n",
        "# get unused features based on generated tree structure\n",
        "feature_list = list( set(feature_set) )\n",
        "print(feature_list)\n",
        "print()\n",
        "\n",
        "drop_list = list( set( list(train_data.columns) ) - set(feature_list) )\n",
        "drop_list.remove('target')\n",
        "print(drop_list)\n",
        "print()\n",
        "\n",
        "# build feature list for second DTree to ensure same performance while rules w/ updated feature indexes;\n",
        "second_tree_features = []\n",
        "count = 0\n",
        "for i in train_data.columns:\n",
        "    if (i != 'target'):\n",
        "        if (i in drop_list):\n",
        "            second_tree_features.append('d_' + str(i) )\n",
        "        else:\n",
        "            second_tree_features.append( 'Feature' + str(count + 1) )\n",
        "            count += 1\n",
        "second_tree_features.append('target')\n",
        "\n",
        "# drop unused features on training & testing set to align with tree\n",
        "train_data = train_data.drop(columns = drop_list)\n",
        "val_data = val_data.drop(columns = drop_list)\n",
        "test_data = test_data.drop(columns = drop_list)\n",
        "\n",
        "# re-index the kept feature names from Feature1\n",
        "num_of_predictors = len( list(train_data.columns) ) - 1 # last one is target variable\n",
        "new_feature_names = []\n",
        "for i in range(num_of_predictors):\n",
        "    new_feature_names.append( 'Feature' + str(i+1) )\n",
        "new_feature_names.append('target')\n",
        "\n",
        "train_data.columns = new_feature_names\n",
        "val_data.columns = new_feature_names\n",
        "test_data.columns = new_feature_names\n",
        "\n",
        "print(train_data.columns)\n",
        "print(second_tree_features)\n",
        "print()\n",
        "\n",
        "print(train_data)\n",
        "print()\n",
        "\n",
        "# with features re-indexed/renamed; generate DTree again\n",
        "train_copy_2.columns = second_tree_features\n",
        "\n",
        "# pre-process training copy to fit with sk-learn decision tree:\n",
        "if (non_ordinal_cat == False):\n",
        "    # for ordinal Cat feature, simply convert to float\n",
        "    for i in train_copy_2.columns:\n",
        "        if (i != 'target'):\n",
        "            train_copy_2[i] = train_copy_2[i].astype('float64')\n",
        "\n",
        "else:\n",
        "    # for non-ordinal Cat feature, perform target encoding\n",
        "    for i in train_copy_2.columns:\n",
        "        if (i != 'target'):\n",
        "            if pd.api.types.is_numeric_dtype(train_copy_2[i]) == False:\n",
        "                encoder = TargetEncoder()\n",
        "                train_copy_2[i] = encoder.fit_transform( train_copy_2[i], train_copy_2['target'] )\n",
        "\n",
        "\n",
        "X_train = train_copy_2.iloc[ :, :len(train_copy.columns)-1 ]\n",
        "Y_train = list( train_copy_2['target'] )\n",
        "\n",
        "clf.fit(X_train, Y_train)\n",
        "\n",
        "# We fit on training set and then evaluate on trainning set again, ONLY to measure whether the tree is able to learn the dataset;\n",
        "preds = clf.predict(X_train)\n",
        "accuracy = metrics.balanced_accuracy_score(Y_train, preds)\n",
        "if accuracy > 0.5:\n",
        "    print('DTree is able to learn this data')\n",
        "print('SciKit-Learn verison: ', sklearn.__version__)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "P-GZ2GMAxbiA"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "_fufjZJh7O7L"
      },
      "outputs": [],
      "source": [
        "if 'sub' not in model_type:\n",
        "\n",
        "    # get dict for kept features for future mapping:\n",
        "    kept_feature_dict = {}\n",
        "    counter = 0\n",
        "    for i in range( len(original_features) ):\n",
        "        s = 'Feature' + str(i + 1)\n",
        "        if s not in drop_list:\n",
        "            counter += 1\n",
        "            kept_feature_dict[counter] = original_features[i]\n",
        "\n",
        "    for k, v in kept_feature_dict.items():\n",
        "\n",
        "        print(k, '  ', train_data[ 'Feature' + str(k) ].nunique(), '  ', v)\n",
        "    print()\n",
        "    print()\n",
        "\n",
        "    # if we would like to match sk-learn DTree adapted by treating cat features as numericals:\n",
        "    if cat_as_num == True:\n",
        "        encoder_dict = {}\n",
        "        # for non-ordinal Cat feature, perform target encoding\n",
        "        for i in train_data.columns:\n",
        "            if (i != 'target'):\n",
        "                if pd.api.types.is_numeric_dtype(train_data[i]) == False:\n",
        "                    encoder = TargetEncoder()\n",
        "                    train_data[i] = encoder.fit_transform( train_data[i], train_data['target'] )\n",
        "                    encoder_dict[i] = encoder\n",
        "\n",
        "    # with DTree clf generated & unused features dropped on both training & testing sets,\n",
        "    # simply iterate through columns to append indexes (starting from 1 for this block) to cat_features list;\n",
        "    cat_features = []\n",
        "    for i in train_data.columns:\n",
        "        if (train_data[i].dtype == 'string' or train_data[i].dtype == 'object'):\n",
        "            print(i)\n",
        "            cat_features.append( int(i[7:]) )\n",
        "\n",
        "    num_of_numerical_predictors = num_of_predictors - len(cat_features)\n",
        "\n",
        "    print(train_data)\n",
        "    print()\n",
        "    print()\n",
        "\n",
        "\n",
        "    rules = get_rules(clf, list(X_train.columns), Y_train)\n",
        "    print('rule pruning --------------------------------------------------')\n",
        "    for i in range( len(rules) ):\n",
        "        for j in range( len(rules[i]) ):\n",
        "            rules[i][j] = rules[i][j].split(' ')\n",
        "            rules[i][j][0] = int( rules[i][j][0][7:] )\n",
        "\n",
        "    # for i in range( len(rules) ):\n",
        "    #     removed_list = []\n",
        "    #     for j in range(len(rules[i]) - 1):\n",
        "    #         if (rules[i][j][0] == rules[i][j+1][0]) and (rules[i][j][1] == rules[i][j+1][1]):\n",
        "    #             removed_list.append(rules[i][j])\n",
        "    #     for elt in removed_list:\n",
        "    #         rules[i].remove(elt)\n",
        "\n",
        "    for i in range( len(rules) ):\n",
        "        removed_list = []\n",
        "        for j in range( len(rules[i]) ):\n",
        "            for num in range(j + 1, len(rules[i]) ):\n",
        "                if (rules[i][j][0] == rules[i][num][0]) and (rules[i][j][1] == rules[i][num][1]):\n",
        "                    if rules[i][j] not in removed_list:\n",
        "                        removed_list.append(rules[i][j])\n",
        "        for elt in removed_list:\n",
        "            rules[i].remove(elt)\n",
        "    # --------------------------------------------------\n",
        "\n",
        "    # change all cat features' symbol to 'be':\n",
        "    for i in range( len(rules) ):\n",
        "        for j in range( len(rules[i]) ):\n",
        "            if rules[i][j][0] in cat_features:\n",
        "                rules[i][j][1] = 'be'\n",
        "\n",
        "    le_node_count = 0\n",
        "    for i in range( len(rules) ):\n",
        "        for j in range( len(rules[i]) ):\n",
        "            if (rules[i][j][1] == '<='):\n",
        "                le_node_count += 1\n",
        "\n",
        "    ge_node_count = 0\n",
        "    for i in range( len(rules) ):\n",
        "        for j in range( len(rules[i]) ):\n",
        "            if (rules[i][j][1] == '>'):\n",
        "                ge_node_count += 1\n",
        "\n",
        "    le_node_counter = 0\n",
        "    ge_node_counter = le_node_count\n",
        "    be_node_counter = le_node_count + ge_node_count\n",
        "\n",
        "    le_nodes = []\n",
        "    ge_nodes = []\n",
        "    be_nodes = []\n",
        "\n",
        "    rule_look_up_indexes = copy.deepcopy(rules)\n",
        "\n",
        "    for i in range( len(rules) ):\n",
        "        for j in range( len(rules[i]) ):\n",
        "            if (rules[i][j][1] == '<='):\n",
        "                rules[i][j][2] = le_node_counter\n",
        "                rule_look_up_indexes[i][j] = le_node_counter\n",
        "                le_nodes.append(rules[i][j])\n",
        "                le_node_counter += 1\n",
        "\n",
        "            elif (rules[i][j][1] == '>'):\n",
        "                rules[i][j][2] = ge_node_counter\n",
        "                rule_look_up_indexes[i][j] = ge_node_counter\n",
        "                ge_nodes.append(rules[i][j])\n",
        "                ge_node_counter += 1\n",
        "\n",
        "            elif (rules[i][j][1] == 'be'):\n",
        "                rules[i][j][2] = be_node_counter\n",
        "                rule_look_up_indexes[i][j] = be_node_counter\n",
        "                be_nodes.append(rules[i][j])\n",
        "                be_node_counter += 1\n",
        "\n",
        "\n",
        "    for rule in rules:\n",
        "        for node in rule:\n",
        "            print(node)\n",
        "        print()\n",
        "\n",
        "    print()\n",
        "    print()\n",
        "    print()\n",
        "\n",
        "    for rule in rule_look_up_indexes:\n",
        "        print(rule)\n",
        "\n",
        "    print()\n",
        "    print()\n",
        "    print()\n",
        "\n",
        "    # perform padding to make all rules of same length for batch processing\n",
        "    for i in range( len(rule_look_up_indexes) ):\n",
        "        if len( rule_look_up_indexes[i] ) < tree_depth:\n",
        "            padding_count = tree_depth - len( rule_look_up_indexes[i] )\n",
        "\n",
        "            for j in range(padding_count):\n",
        "                rule_look_up_indexes[i].append(be_node_counter)\n",
        "\n",
        "    for rule in rule_look_up_indexes:\n",
        "        print(rule)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "MdiAWIFyi0ug"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hfT6STAA3NE7"
      },
      "outputs": [],
      "source": [
        "if 'sub' in model_type:\n",
        "\n",
        "    # Add rules from differrent subtrees\n",
        "\n",
        "    # get dict for kept features for future mapping:\n",
        "    kept_feature_dict = {}\n",
        "    counter = 0\n",
        "    for i in range( len(original_features) ):\n",
        "        s = 'Feature' + str(i + 1)\n",
        "        if s not in drop_list:\n",
        "            counter += 1\n",
        "            kept_feature_dict[counter] = original_features[i]\n",
        "\n",
        "    for k, v in kept_feature_dict.items():\n",
        "\n",
        "        print(k, '  ', train_data[ 'Feature' + str(k) ].nunique(), '  ', v)\n",
        "    print()\n",
        "    print()\n",
        "\n",
        "    # if we would like to match sk-learn DTree adapted by treating cat features as numericals:\n",
        "    if cat_as_num == True:\n",
        "        encoder_dict = {}\n",
        "        # for non-ordinal Cat feature, perform target encoding\n",
        "        for i in train_data.columns:\n",
        "            if (i != 'target'):\n",
        "                if pd.api.types.is_numeric_dtype(train_data[i]) == False:\n",
        "                    encoder = TargetEncoder()\n",
        "                    train_data[i] = encoder.fit_transform( train_data[i], train_data['target'] )\n",
        "                    encoder_dict[i] = encoder\n",
        "\n",
        "    # with DTree clf generated & unused features dropped on both training & testing sets,\n",
        "    # simply iterate through columns to append indexes (starting from 1 for this block) to cat_features list;\n",
        "    cat_features = []\n",
        "    for i in train_data.columns:\n",
        "        if (train_data[i].dtype == 'string' or train_data[i].dtype == 'object'):\n",
        "            print(i)\n",
        "            cat_features.append( int(i[7:]) )\n",
        "\n",
        "    num_of_numerical_predictors = num_of_predictors - len(cat_features)\n",
        "\n",
        "    print(train_data)\n",
        "    print()\n",
        "    print()\n",
        "\n",
        "    rules = get_rules(clf, list(X_train.columns), Y_train)\n",
        "    print('rule pruning --------------------------------------------------')\n",
        "    for i in range( len(rules) ):\n",
        "        for j in range( len(rules[i]) ):\n",
        "            rules[i][j] = rules[i][j].split(' ')\n",
        "            rules[i][j][0] = int( rules[i][j][0][7:] )\n",
        "\n",
        "    for i in range( len(rules) ):\n",
        "        removed_list = []\n",
        "        for j in range( len(rules[i]) ):\n",
        "            for num in range(j + 1, len(rules[i]) ):\n",
        "                if (rules[i][j][0] == rules[i][num][0]) and (rules[i][j][1] == rules[i][num][1]):\n",
        "                    if rules[i][j] not in removed_list:\n",
        "                        removed_list.append(rules[i][j])\n",
        "        for elt in removed_list:\n",
        "            rules[i].remove(elt)\n",
        "    # --------------------------------------------------\n",
        "\n",
        "\n",
        "    # Add subtrees on train_data w/ kept features only:\n",
        "\n",
        "    # make 3 copies of train & test sets for later use;\n",
        "    train_copy = train_data.sample(frac=0.8, replace = False, random_state = 0)\n",
        "    train_copy_2 = train_data.sample(frac=0.8, replace = False, random_state = 1)\n",
        "    train_copy_3 = train_data.sample(frac=0.8, replace = False, random_state = 2)\n",
        "\n",
        "    # pre-process training copy to fit with sk-learn decision tree:\n",
        "    if (non_ordinal_cat == False):\n",
        "        # for ordinal Cat feature, simply convert to float\n",
        "        for i in train_copy.columns:\n",
        "            if (i != 'target'):\n",
        "                train_copy[i] = train_copy[i].astype('float64')\n",
        "                train_copy_2[i] = train_copy_2[i].astype('float64')\n",
        "                train_copy_3[i] = train_copy_3[i].astype('float64')\n",
        "\n",
        "    else:\n",
        "        # for non-ordinal Cat feature, perform target encoding\n",
        "        for i in train_copy.columns:\n",
        "            if (i != 'target'):\n",
        "                if pd.api.types.is_numeric_dtype(train_copy[i]) == False:\n",
        "                    print(i)\n",
        "                    encoder = TargetEncoder()\n",
        "                    train_copy[i] = encoder.fit_transform( train_copy[i], train_copy['target'] )\n",
        "                    encoder = TargetEncoder()\n",
        "                    train_copy_2[i] = encoder.fit_transform( train_copy_2[i], train_copy_2['target'] )\n",
        "                    encoder = TargetEncoder()\n",
        "                    train_copy_3[i] = encoder.fit_transform( train_copy_3[i], train_copy_3['target'] )\n",
        "\n",
        "\n",
        "    # compute & show performance for DTree on processed data\n",
        "    X_train = train_copy.iloc[ :, :len(train_copy.columns) - 1 ]\n",
        "    Y_train = list( train_copy['target'] )\n",
        "\n",
        "    clf_minus_2 = DecisionTreeClassifier(max_depth = tree_depth - 2, class_weight = 'balanced', random_state = 1)\n",
        "    clf_minus_2.fit(X_train, Y_train)\n",
        "\n",
        "    rules_minus_2 = get_rules(clf_minus_2, list(X_train.columns), Y_train)\n",
        "    print('rule pruning --------------------------------------------------')\n",
        "    for i in range( len(rules_minus_2) ):\n",
        "        for j in range( len(rules_minus_2[i]) ):\n",
        "            rules_minus_2[i][j] = rules_minus_2[i][j].split(' ')\n",
        "            rules_minus_2[i][j][0] = int( rules_minus_2[i][j][0][7:] )\n",
        "\n",
        "    for i in range( len(rules) ):\n",
        "        removed_list = []\n",
        "        for j in range( len(rules[i]) ):\n",
        "            for num in range(j + 1, len(rules[i]) ):\n",
        "                if (rules[i][j][0] == rules[i][num][0]) and (rules[i][j][1] == rules[i][num][1]):\n",
        "                    if rules[i][j] not in removed_list:\n",
        "                        removed_list.append(rules[i][j])\n",
        "        for elt in removed_list:\n",
        "            rules[i].remove(elt)\n",
        "\n",
        "\n",
        "    X_train = train_copy_2.iloc[ :, :len(train_copy_2.columns) - 1 ]\n",
        "    Y_train = list( train_copy_2['target'] )\n",
        "\n",
        "    clf_minus_4 = DecisionTreeClassifier(max_depth = tree_depth - 4, class_weight = 'balanced', random_state = 1)\n",
        "    clf_minus_4.fit(X_train, Y_train)\n",
        "\n",
        "    rules_minus_4 = get_rules(clf_minus_4, list(X_train.columns), Y_train)\n",
        "    print('rule pruning --------------------------------------------------')\n",
        "    for i in range( len(rules_minus_4) ):\n",
        "        for j in range( len(rules_minus_4[i]) ):\n",
        "            rules_minus_4[i][j] = rules_minus_4[i][j].split(' ')\n",
        "            rules_minus_4[i][j][0] = int( rules_minus_4[i][j][0][7:] )\n",
        "\n",
        "    for i in range( len(rules) ):\n",
        "        removed_list = []\n",
        "        for j in range( len(rules[i]) ):\n",
        "            for num in range(j + 1, len(rules[i]) ):\n",
        "                if (rules[i][j][0] == rules[i][num][0]) and (rules[i][j][1] == rules[i][num][1]):\n",
        "                    if rules[i][j] not in removed_list:\n",
        "                        removed_list.append(rules[i][j])\n",
        "        for elt in removed_list:\n",
        "            rules[i].remove(elt)\n",
        "\n",
        "\n",
        "    X_train = train_copy_3.iloc[ :, :len(train_copy_3.columns) - 1 ]\n",
        "    Y_train = list( train_copy_3['target'] )\n",
        "\n",
        "    clf_minus_6 = DecisionTreeClassifier(max_depth = tree_depth - 6, class_weight = 'balanced', random_state = 1)\n",
        "    clf_minus_6.fit(X_train, Y_train)\n",
        "\n",
        "    rules_minus_6 = get_rules(clf_minus_6, list(X_train.columns), Y_train)\n",
        "    print('rule pruning --------------------------------------------------')\n",
        "    for i in range( len(rules_minus_6) ):\n",
        "        for j in range( len(rules_minus_6[i]) ):\n",
        "            rules_minus_6[i][j] = rules_minus_6[i][j].split(' ')\n",
        "            rules_minus_6[i][j][0] = int( rules_minus_6[i][j][0][7:] )\n",
        "\n",
        "    for i in range( len(rules) ):\n",
        "        removed_list = []\n",
        "        for j in range( len(rules[i]) ):\n",
        "            for num in range(j + 1, len(rules[i]) ):\n",
        "                if (rules[i][j][0] == rules[i][num][0]) and (rules[i][j][1] == rules[i][num][1]):\n",
        "                    if rules[i][j] not in removed_list:\n",
        "                        removed_list.append(rules[i][j])\n",
        "        for elt in removed_list:\n",
        "            rules[i].remove(elt)\n",
        "\n",
        "    rules = rules + rules_minus_2 + rules_minus_4 + rules_minus_6 # merge rules from the main tree & its subtrees\n",
        "\n",
        "    # change all cat features' symbol to 'be':\n",
        "    for i in range( len(rules) ):\n",
        "        for j in range( len(rules[i]) ):\n",
        "            if rules[i][j][0] in cat_features:\n",
        "                rules[i][j][1] = 'be'\n",
        "\n",
        "    le_node_count = 0\n",
        "    for i in range( len(rules) ):\n",
        "        for j in range( len(rules[i]) ):\n",
        "            if (rules[i][j][1] == '<='):\n",
        "                le_node_count += 1\n",
        "\n",
        "    ge_node_count = 0\n",
        "    for i in range( len(rules) ):\n",
        "        for j in range( len(rules[i]) ):\n",
        "            if (rules[i][j][1] == '>'):\n",
        "                ge_node_count += 1\n",
        "\n",
        "    le_node_counter = 0\n",
        "    ge_node_counter = le_node_count\n",
        "    be_node_counter = le_node_count + ge_node_count\n",
        "\n",
        "    le_nodes = []\n",
        "    ge_nodes = []\n",
        "    be_nodes = []\n",
        "\n",
        "    rule_look_up_indexes = copy.deepcopy(rules)\n",
        "\n",
        "    for i in range( len(rules) ):\n",
        "        for j in range( len(rules[i]) ):\n",
        "            if (rules[i][j][1] == '<='):\n",
        "                rules[i][j][2] = le_node_counter\n",
        "                rule_look_up_indexes[i][j] = le_node_counter\n",
        "                le_nodes.append(rules[i][j])\n",
        "                le_node_counter += 1\n",
        "\n",
        "            elif (rules[i][j][1] == '>'):\n",
        "                rules[i][j][2] = ge_node_counter\n",
        "                rule_look_up_indexes[i][j] = ge_node_counter\n",
        "                ge_nodes.append(rules[i][j])\n",
        "                ge_node_counter += 1\n",
        "\n",
        "            elif (rules[i][j][1] == 'be'):\n",
        "                rules[i][j][2] = be_node_counter\n",
        "                rule_look_up_indexes[i][j] = be_node_counter\n",
        "                be_nodes.append(rules[i][j])\n",
        "                be_node_counter += 1\n",
        "\n",
        "\n",
        "    for rule in rules:\n",
        "        for node in rule:\n",
        "            print(node)\n",
        "        print()\n",
        "\n",
        "    print()\n",
        "    print()\n",
        "    print()\n",
        "\n",
        "    for rule in rule_look_up_indexes:\n",
        "        print(rule)\n",
        "\n",
        "    print()\n",
        "    print()\n",
        "    print()\n",
        "\n",
        "    # perform padding to make all rules of same length for batch processing\n",
        "    for i in range( len(rule_look_up_indexes) ):\n",
        "        if len( rule_look_up_indexes[i] ) < tree_depth:\n",
        "            padding_count = tree_depth - len( rule_look_up_indexes[i] )\n",
        "\n",
        "            for j in range(padding_count):\n",
        "                rule_look_up_indexes[i].append(be_node_counter)\n",
        "\n",
        "    for rule in rule_look_up_indexes:\n",
        "        print(rule)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "5IpFHWPK3NYE"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "v362SmGBHdQ7"
      },
      "outputs": [],
      "source": [
        "def FeatureTransfers(train_data, bins): # change bin_number to a map (feature, bin_number)\n",
        "\n",
        "    df = train_data.copy()\n",
        "    df_2 = train_data.copy()\n",
        "\n",
        "    print( len(df) )\n",
        "\n",
        "    cut_off_dict = {}\n",
        "\n",
        "    feature_level_index = 0\n",
        "    feature_lookup_dict = {} # lookup dict for index access of the embedding matrix\n",
        "    cat_bins_total = []\n",
        "\n",
        "    # Use info from training set only for conversion on both sets to avoid data leakage\n",
        "    for i in df.columns:\n",
        "        if ('target' in i) == False:\n",
        "            if pd.api.types.is_numeric_dtype(df[i]):\n",
        "                levels = df[i].nunique()\n",
        "\n",
        "                if (levels > bins): # Cont. Num. feature; compute bins\n",
        "                    cut_offs = []\n",
        "                    for num in range(0, bins + 1):\n",
        "                        n = num * (1.0 / float(bins) ) # n ranges from 0.0 - 1.0 to cover both ends\n",
        "                        cut_offs.append( df[i].quantile(n) )\n",
        "                    if (len( set(cut_offs) ) == 2):\n",
        "                        cut_offs.insert(0, df[i].min() - 0.01)\n",
        "\n",
        "                else: # if <= bin number, just use original bins, no need to compute\n",
        "                    cut_offs = list( df[i].unique() )\n",
        "                    cut_offs.append(df[i].min() - 0.01)# insert an extra bin right after 1st bin; s/t every bin will be a unique level\n",
        "                    cut_offs.sort() # sort bins as they need to increase monotonically\n",
        "\n",
        "                cut_off_dict[i] = cut_offs\n",
        "\n",
        "                # For testing set\n",
        "                # #  1st cutoff  convert  = 1st cutoff\n",
        "                # #  last cutoff  convert  = last cutoff\n",
        "                # df_test.loc[df_test[i] < cut_offs[0], i] = cut_offs[0]\n",
        "                # df_test.loc[df_test[i] > cut_offs[len(cut_offs)-1], i] = cut_offs[len(cut_offs)-1] ------------------------- call later\n",
        "\n",
        "                # use cut-offs generated from only train set to perform conversion to avoid leakage\n",
        "                df[i] = pd.cut(df[i], cut_offs, duplicates = 'drop', right = True, include_lowest = True)\n",
        "                # df_test[i] = pd.cut(df_test[i], cut_offs, duplicates = 'drop', right = True, include_lowest = True) ------------------------- call later\n",
        "\n",
        "                feature_copy = df[i].copy()\n",
        "\n",
        "                df[i] = i + '_' + df[i].astype(str)\n",
        "                # df_test[i] = i + '_' + df_test[i].astype(str) ------------------------- call later\n",
        "\n",
        "                feature_levels = list( feature_copy.unique() )\n",
        "\n",
        "                feature_levels.sort() # sort numerical bins in ascending order\n",
        "                for count in range( len(feature_levels) ):\n",
        "                    feature_levels[count] = i + '_' + str( feature_levels[count] )\n",
        "\n",
        "                for i in feature_levels:\n",
        "                    feature_lookup_dict[i] = feature_level_index\n",
        "                    feature_level_index += 1\n",
        "\n",
        "                if ( len( set(cut_offs) ) <= bins ):\n",
        "                    feature_level_index += bins - len( set(cut_offs) ) + 1\n",
        "                # if ( len(feature_levels) <= bins ):\n",
        "                #     feature_level_index += bins - len(feature_levels)\n",
        "\n",
        "\n",
        "    for i in df_2.columns:\n",
        "        if ('target' in i) == False:\n",
        "            if pd.api.types.is_numeric_dtype(df_2[i]) == False:\n",
        "\n",
        "                # for Cat features:\n",
        "                feature_copy = df_2[i].copy()\n",
        "                feature_levels = list( feature_copy.unique() )\n",
        "                cat_bins_total.append( len(feature_levels) )\n",
        "\n",
        "                df[i] = i + '_' + df[i].astype(str)\n",
        "                # df_test[i] = i + '_' + df_test[i].astype(str) ------------------------- call later\n",
        "\n",
        "                for count in range( len(feature_levels) ):\n",
        "                    feature_levels[count] = i + '_' + str( feature_levels[count] )\n",
        "\n",
        "                for i in feature_levels:\n",
        "                    feature_lookup_dict[i] = feature_level_index\n",
        "                    feature_level_index += 1\n",
        "\n",
        "\n",
        "    return [df, cut_off_dict, feature_lookup_dict, cat_bins_total]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "oYWTIBzDHdVb"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "j5DaDXaBv4VZ"
      },
      "outputs": [],
      "source": [
        "def NSDT_modeling_preps(bins, train_loader, new_train, cut_off_dict, feature_lookup_dict, cat_bins_total, total_numerical_nodes, total_levels):\n",
        "\n",
        "    train_loader = train_loader\n",
        "    new_train = new_train\n",
        "    cut_off_dict = cut_off_dict\n",
        "    feature_lookup_dict = feature_lookup_dict\n",
        "    cat_bins_total = cat_bins_total\n",
        "    total_numerical_nodes = total_numerical_nodes\n",
        "    total_levels = total_levels\n",
        "\n",
        "    print( len(new_train) )\n",
        "\n",
        "\n",
        "    # Create context value index lists for training set:\n",
        "    # be nodes would simply be empty if the dataset contains num features only\n",
        "    all_batch_le_context_value_index_list = []\n",
        "    all_batch_ge_context_value_index_list = []\n",
        "    all_batch_be_context_value_index_list = []\n",
        "\n",
        "    for batch in train_loader:\n",
        "\n",
        "        batch_size = len(batch)\n",
        "        batch_le_context_value_index_list = []\n",
        "        batch_ge_context_value_index_list = []\n",
        "        batch_be_context_value_index_list = []\n",
        "\n",
        "        for node in le_nodes:\n",
        "            le_context_value_index_list = [ node[2] ] * batch_size\n",
        "            batch_le_context_value_index_list.append(le_context_value_index_list)\n",
        "\n",
        "        for node in ge_nodes:\n",
        "            ge_context_value_index_list = [ node[2] ] * batch_size\n",
        "            batch_ge_context_value_index_list.append(ge_context_value_index_list)\n",
        "\n",
        "        for node in be_nodes:\n",
        "            be_context_value_index_list = [ node[2] ] * batch_size\n",
        "            batch_be_context_value_index_list.append(be_context_value_index_list)\n",
        "\n",
        "        all_batch_le_context_value_index_list.append(batch_le_context_value_index_list)\n",
        "        all_batch_ge_context_value_index_list.append(batch_ge_context_value_index_list)\n",
        "        all_batch_be_context_value_index_list.append(batch_be_context_value_index_list)\n",
        "\n",
        "\n",
        "    feature_indexes_le_nodes  = []\n",
        "    feature_indexes_ge_nodes  = []\n",
        "    feature_indexes_be_nodes  = []\n",
        "\n",
        "    for node in le_nodes:\n",
        "        feature_indexes_le_nodes.append(node[0] - 1)\n",
        "\n",
        "    for node in ge_nodes:\n",
        "        feature_indexes_ge_nodes.append(node[0] - 1)\n",
        "\n",
        "    for node in be_nodes:\n",
        "        feature_indexes_be_nodes.append(node[0] - 1)\n",
        "\n",
        "\n",
        "    # le & ge regs:\n",
        "    feature_bin_counts = [bins] * num_of_numerical_predictors\n",
        "\n",
        "    le_current_feature_indexes = []\n",
        "\n",
        "    for list_count in range(bins,0,-1):\n",
        "        current_index = 0\n",
        "        twoD_list = []\n",
        "\n",
        "        for feature_counts in feature_bin_counts:\n",
        "            oneD_list = [current_index] * list_count\n",
        "            twoD_list.append(oneD_list)\n",
        "\n",
        "            current_index += feature_counts\n",
        "        le_current_feature_indexes.append(twoD_list)\n",
        "\n",
        "    for i in range(bins):\n",
        "        le_current_feature_indexes[i] = list( np.asarray(le_current_feature_indexes[i]) + i )\n",
        "\n",
        "\n",
        "    le_next_feature_indexes = []\n",
        "\n",
        "    for list_count in range(bins,0,-1):\n",
        "        current_index = 0\n",
        "        twoD_list = []\n",
        "\n",
        "        for feature_counts in feature_bin_counts:\n",
        "            oneD_list = list( range(current_index, list_count + current_index) )\n",
        "            twoD_list.append(oneD_list)\n",
        "\n",
        "            current_index += feature_counts\n",
        "        le_next_feature_indexes.append(twoD_list)\n",
        "\n",
        "    for i in range(bins):\n",
        "        le_next_feature_indexes[i] = list( np.asarray(le_next_feature_indexes[i]) + i )\n",
        "\n",
        "\n",
        "    for i in range( len(le_current_feature_indexes) ):\n",
        "        le_current_feature_indexes[i] = torch.tensor(le_current_feature_indexes[i], dtype = torch.long).to('cuda')\n",
        "\n",
        "    for i in range( len(le_next_feature_indexes) ):\n",
        "        le_next_feature_indexes[i] = torch.tensor(le_next_feature_indexes[i], dtype = torch.long).to('cuda')\n",
        "\n",
        "\n",
        "    print( le_current_feature_indexes[0][0] )\n",
        "    print( le_next_feature_indexes[0][1] )\n",
        "    print()\n",
        "\n",
        "\n",
        "    ge_current_feature_indexes = []\n",
        "\n",
        "    for list_count in range(1, bins + 1):\n",
        "        current_index = 0\n",
        "        twoD_list = []\n",
        "\n",
        "        for feature_counts in feature_bin_counts:\n",
        "            oneD_list = [current_index] * list_count\n",
        "            twoD_list.append(oneD_list)\n",
        "\n",
        "            current_index += feature_counts\n",
        "        ge_current_feature_indexes.append(twoD_list)\n",
        "\n",
        "    for i in range(bins):\n",
        "        ge_current_feature_indexes[i] = list( np.asarray(ge_current_feature_indexes[i]) + i )\n",
        "\n",
        "\n",
        "    ge_next_feature_indexes = []\n",
        "\n",
        "    for list_count in range(1,bins + 1):\n",
        "        current_index = 0\n",
        "        twoD_list = []\n",
        "\n",
        "        for feature_counts in feature_bin_counts:\n",
        "            oneD_list = list( range(current_index, list_count + current_index) )\n",
        "            twoD_list.append(oneD_list)\n",
        "\n",
        "            current_index += feature_counts\n",
        "        ge_next_feature_indexes.append(twoD_list)\n",
        "\n",
        "\n",
        "    for i in range( len(ge_current_feature_indexes) ):\n",
        "        ge_current_feature_indexes[i] = torch.tensor(ge_current_feature_indexes[i], dtype = torch.long).to('cuda')\n",
        "\n",
        "    for i in range( len(ge_next_feature_indexes) ):\n",
        "        ge_next_feature_indexes[i] = torch.tensor(ge_next_feature_indexes[i], dtype = torch.long).to('cuda')\n",
        "\n",
        "\n",
        "    print( (ge_current_feature_indexes[0][0]) )\n",
        "    print( (ge_current_feature_indexes[0][1]) )\n",
        "    print()\n",
        "\n",
        "    print( (ge_current_feature_indexes[1][0]) )\n",
        "    print( (ge_current_feature_indexes[1][1]) )\n",
        "    print()\n",
        "\n",
        "\n",
        "    # compute le & ge x matrices for regularization:\n",
        "    one_bin_diff = round(0.5 / (bins - 1), 6)\n",
        "    le_expected_outputs_1 = []\n",
        "    le_expected_outputs_2 = []\n",
        "\n",
        "    for i in range(bins): # all features' 1st bin to last (25th) bin\n",
        "        bin_list_of_outputs = []\n",
        "\n",
        "        for j in range(len (le_current_feature_indexes[i]) ):\n",
        "            count = 0\n",
        "            list_of_outputs = []\n",
        "\n",
        "            for k in range(len (le_current_feature_indexes[i][0]) ):\n",
        "                list_of_outputs.append(0.5 + count * one_bin_diff)\n",
        "                count += 1\n",
        "\n",
        "            bin_list_of_outputs.append(list_of_outputs)\n",
        "\n",
        "        le_expected_outputs_1.append( torch.FloatTensor(bin_list_of_outputs).to('cuda') ) # bin_list_of_outputs = 21 x 25 for the first bin\n",
        "\n",
        "\n",
        "    for i in range(bins): # all features' 1st bin to last (25th) bin\n",
        "        bin_list_of_outputs = []\n",
        "\n",
        "        for j in range(len (le_current_feature_indexes[i]) ):\n",
        "            count = 0\n",
        "            list_of_outputs = []\n",
        "\n",
        "            for k in range(len (le_current_feature_indexes[i][0]) ):\n",
        "                list_of_outputs.append(0.5 - count * one_bin_diff)\n",
        "                count += 1\n",
        "\n",
        "            bin_list_of_outputs.append(list_of_outputs)\n",
        "\n",
        "        le_expected_outputs_2.append( torch.FloatTensor(bin_list_of_outputs).to('cuda') )\n",
        "\n",
        "\n",
        "    print((le_expected_outputs_1[2][0]))\n",
        "    print((le_expected_outputs_2[2][0]))\n",
        "\n",
        "    # [2 2]\n",
        "    # [0, 1]\n",
        "\n",
        "    # [24 24 24 24 24 24 24 24 24 24 24 24 24 24 24 24 24 24 24 24 24 24 24 24]\n",
        "    # [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23]\n",
        "\n",
        "    ge_expected_outputs_1 = []\n",
        "    ge_expected_outputs_2 = []\n",
        "\n",
        "    for i in range(bins): # all features' 1st bin to last (25th) bin\n",
        "        bin_list_of_outputs = []\n",
        "\n",
        "        for j in range(len (ge_current_feature_indexes[i]) ):\n",
        "            count = len (ge_current_feature_indexes[i][0]) - 1\n",
        "            list_of_outputs = []\n",
        "\n",
        "            for k in range(len (ge_current_feature_indexes[i][0]) ):\n",
        "                list_of_outputs.append(0.5 + count * one_bin_diff)\n",
        "                count -= 1\n",
        "\n",
        "            bin_list_of_outputs.append(list_of_outputs)\n",
        "\n",
        "        ge_expected_outputs_1.append( torch.FloatTensor(bin_list_of_outputs).to('cuda') )\n",
        "\n",
        "\n",
        "    for i in range(bins): # all features' 1st bin to last (25th) bin\n",
        "        bin_list_of_outputs = []\n",
        "\n",
        "        for j in range(len (ge_current_feature_indexes[i]) ):\n",
        "            count = len (ge_current_feature_indexes[i][0]) - 1\n",
        "            list_of_outputs = []\n",
        "\n",
        "            for k in range(len (ge_current_feature_indexes[i][0]) ):\n",
        "                list_of_outputs.append(0.5 - count * one_bin_diff)\n",
        "                count -= 1\n",
        "\n",
        "            bin_list_of_outputs.append(list_of_outputs)\n",
        "\n",
        "        ge_expected_outputs_2.append( torch.FloatTensor(bin_list_of_outputs).to('cuda') )\n",
        "\n",
        "\n",
        "    print((ge_expected_outputs_1[5][0]))\n",
        "    print((ge_expected_outputs_2[5][0]))\n",
        "\n",
        "\n",
        "    # ref, Si (asym & trans), and ordering_Si;\n",
        "    le_ref_lookup_indexes = []\n",
        "    ge_ref_lookup_indexes = []\n",
        "\n",
        "    le_features_Si_indexes = []\n",
        "    ge_features_Si_indexes = []\n",
        "\n",
        "    le_orderings_Si_indexes = []\n",
        "    ge_orderings_Si_indexes = []\n",
        "\n",
        "    num_features = []\n",
        "    for i in range(len(new_train.columns) - 1): # target feature at the end\n",
        "        num_features.append(i)\n",
        "\n",
        "    for i in cat_features:\n",
        "        num_features.remove(i - 1)\n",
        "\n",
        "    feature_bin_count = 0\n",
        "    for i in range( len(num_features) ):\n",
        "        feature_Si_indexes = []\n",
        "        feature_bin_indexes = []\n",
        "        feature_context_indexes = []\n",
        "\n",
        "        feature_bin_indexes = list( range(feature_bin_count, feature_bin_count + bins) )\n",
        "        feature_bin_count += bins\n",
        "\n",
        "        for rule in rules:\n",
        "            for node in rule:\n",
        "                if (node[1] == '<='):\n",
        "                    if (node[0] == num_features[i] + 1):\n",
        "                        feature_context_indexes.append(node[2])\n",
        "                        le_ref_lookup_indexes.append(node[2])\n",
        "\n",
        "        print(i, ': ', feature_context_indexes)\n",
        "\n",
        "        feature_Si_indexes.append( torch.LongTensor(feature_bin_indexes).to('cuda') )\n",
        "        feature_Si_indexes.append( torch.LongTensor(feature_context_indexes).to('cuda') )\n",
        "        le_features_Si_indexes.append(feature_Si_indexes)\n",
        "\n",
        "        for context_value in feature_context_indexes:\n",
        "            ordering_Si_indexes = []\n",
        "            ordering_Si_indexes.append( torch.LongTensor(feature_bin_indexes).to('cuda') )\n",
        "            ordering_Si_indexes.append( torch.LongTensor( [context_value] * bins ).to('cuda') )\n",
        "            le_orderings_Si_indexes.append(ordering_Si_indexes)\n",
        "\n",
        "    print()\n",
        "    print()\n",
        "\n",
        "    feature_bin_count = 0\n",
        "    for i in range( len(num_features) ):\n",
        "        feature_Si_indexes = []\n",
        "        feature_bin_indexes = []\n",
        "        feature_context_indexes = []\n",
        "\n",
        "        feature_bin_indexes = list( range(feature_bin_count, feature_bin_count + bins) )\n",
        "        feature_bin_count += bins\n",
        "\n",
        "        for rule in rules:\n",
        "            for node in rule:\n",
        "                if (node[1] == '>'):\n",
        "                    if (node[0] == num_features[i] + 1):\n",
        "                        feature_context_indexes.append(node[2])\n",
        "                        ge_ref_lookup_indexes.append(node[2])\n",
        "\n",
        "        print(i, ': ', feature_context_indexes)\n",
        "\n",
        "        feature_Si_indexes.append( torch.LongTensor(feature_bin_indexes).to('cuda') )\n",
        "        feature_Si_indexes.append( torch.LongTensor(feature_context_indexes).to('cuda') )\n",
        "        ge_features_Si_indexes.append(feature_Si_indexes)\n",
        "\n",
        "        for context_value in feature_context_indexes:\n",
        "            ordering_Si_indexes = []\n",
        "            ordering_Si_indexes.append( torch.LongTensor(feature_bin_indexes).to('cuda') )\n",
        "            ordering_Si_indexes.append( torch.LongTensor( [context_value] * bins ).to('cuda') )\n",
        "            ge_orderings_Si_indexes.append(ordering_Si_indexes)\n",
        "\n",
        "    print( len(le_features_Si_indexes), len(ge_features_Si_indexes) )\n",
        "    print( len(le_orderings_Si_indexes), len(ge_orderings_Si_indexes) )\n",
        "\n",
        "    le_ref_lookup_indexes = torch.LongTensor(le_ref_lookup_indexes).to('cuda')\n",
        "    ge_ref_lookup_indexes = torch.LongTensor(ge_ref_lookup_indexes).to('cuda')\n",
        "\n",
        "\n",
        "    # Be reg prep:\n",
        "    # For each Cat. feature, we need: feature bin indexes & context value indexes;\n",
        "    # Create a 2D list per feature; all feature bin index (as 1D list) * # of context value indexes times/rows;\n",
        "    # Create another 2D list per feature: each 1D list = a context value index * cardinality of feature;\n",
        "    # num_of_numerical_predictors * bins = 1st cat\n",
        "\n",
        "    if len(cat_features) > 0:\n",
        "        # make feature bin lookup indexes for all cat features (not needed for num features, always 25):\n",
        "        cat_features_bin_indexes = []\n",
        "\n",
        "        i = num_of_numerical_predictors * bins\n",
        "        count = 0\n",
        "        while i < total_levels:\n",
        "            cat_feature_bin_indexes = []\n",
        "            for j in range(cat_bins_total[count]):\n",
        "                cat_feature_bin_indexes.append(i)\n",
        "                i += 1\n",
        "\n",
        "            cat_features_bin_indexes.append(cat_feature_bin_indexes)\n",
        "            count += 1\n",
        "\n",
        "\n",
        "        be_Si_indexes = []\n",
        "        be_Si_indexes_2 = []\n",
        "        for i in range( len(cat_features) ):\n",
        "            be_bin_indexes = []\n",
        "            be_context_indexes = []\n",
        "\n",
        "            be_bin_indexes = cat_features_bin_indexes[i]\n",
        "\n",
        "            for rule in rules:\n",
        "                for node in rule:\n",
        "                    if (node[1] == 'be'):\n",
        "                        if (node[0] == cat_features[i]):\n",
        "                            be_context_indexes.append(node[2])\n",
        "\n",
        "            # print(be_bin_indexes, be_context_indexes)\n",
        "            be_context_indexes = list( set(be_context_indexes) )\n",
        "            # print(be_bin_indexes, be_context_indexes)\n",
        "\n",
        "            for j in be_context_indexes:\n",
        "                be_Si_indexes.append( [ be_bin_indexes, [j] * len(be_bin_indexes) ] )\n",
        "                be_Si_indexes_2.append( [ be_bin_indexes, [j] * len(be_bin_indexes) ] )\n",
        "\n",
        "        # for speed boost, may append all i with same i[0] into 1 list;\n",
        "        for i in range( len(be_Si_indexes) ):\n",
        "            be_Si_indexes[i][0] = torch.LongTensor(be_Si_indexes[i][0]).to('cuda')\n",
        "            be_Si_indexes[i][1] = torch.LongTensor(be_Si_indexes[i][1]).to('cuda')\n",
        "\n",
        "\n",
        "        cat_features_Si_indexes = []\n",
        "        counter = 0\n",
        "        for i in range( len(cat_features) ):\n",
        "\n",
        "            be_bin_indexes = []\n",
        "            be_context_indexes = []\n",
        "            feature = be_Si_indexes_2[counter][0]\n",
        "\n",
        "            for j in be_Si_indexes_2:\n",
        "                if feature == j[0]:\n",
        "                    be_bin_indexes.append(j[0])\n",
        "                    be_context_indexes.append(j[1])\n",
        "                    counter += 1\n",
        "\n",
        "            be_bin_indexes = torch.LongTensor(be_bin_indexes).to('cuda')\n",
        "            be_context_indexes = torch.LongTensor(be_context_indexes).to('cuda')\n",
        "            cat_features_Si_indexes.append( [be_bin_indexes, be_context_indexes] )\n",
        "\n",
        "    if len(cat_features) > 0:\n",
        "        return feature_indexes_le_nodes, feature_indexes_ge_nodes, feature_indexes_be_nodes, all_batch_le_context_value_index_list, \\\n",
        "        all_batch_ge_context_value_index_list, all_batch_be_context_value_index_list, cat_features_Si_indexes, le_orderings_Si_indexes, \\\n",
        "        ge_orderings_Si_indexes, le_ref_lookup_indexes, le_features_Si_indexes, ge_ref_lookup_indexes, ge_features_Si_indexes, le_current_feature_indexes, \\\n",
        "        le_next_feature_indexes, le_expected_outputs_1, le_expected_outputs_2, ge_current_feature_indexes, ge_next_feature_indexes, ge_expected_outputs_1, \\\n",
        "        ge_expected_outputs_2\n",
        "\n",
        "    else:\n",
        "        return feature_indexes_le_nodes, feature_indexes_ge_nodes, feature_indexes_be_nodes, all_batch_le_context_value_index_list, \\\n",
        "        all_batch_ge_context_value_index_list, all_batch_be_context_value_index_list, le_orderings_Si_indexes, \\\n",
        "        ge_orderings_Si_indexes, le_ref_lookup_indexes, le_features_Si_indexes, ge_ref_lookup_indexes, ge_features_Si_indexes, le_current_feature_indexes, \\\n",
        "        le_next_feature_indexes, le_expected_outputs_1, le_expected_outputs_2, ge_current_feature_indexes, ge_next_feature_indexes, ge_expected_outputs_1, \\\n",
        "        ge_expected_outputs_2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "pDzEkEfgv5Pk"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AdjYhplzvKTM"
      },
      "outputs": [],
      "source": [
        "def objective(trial):\n",
        "\n",
        "    torch_seed = 0\n",
        "    torch.manual_seed(torch_seed)\n",
        "\n",
        "    # You may also tune epoch as a hyper-param by adding it below\n",
        "    if (dataset == 'Higgs'):\n",
        "        epochs = 150\n",
        "        p = 40\n",
        "    elif (dataset == 'Cover'):\n",
        "        epochs = 40\n",
        "        p = 15\n",
        "    else:\n",
        "        epochs = 30\n",
        "        p = 10\n",
        "    early_stopper = EarlyStopper(patience = p, min_delta = 0)\n",
        "\n",
        "    # set hyper-params to tune: -------------------------------------------\n",
        "    global bayes_opt\n",
        "    bayes_opt = True\n",
        "\n",
        "    normalize = trial.suggest_categorical( 'normalize num features', ['0', '1'] )\n",
        "\n",
        "    if (dataset == 'Higgs'):\n",
        "        batch_size = trial.suggest_int('training batch size', 128, 256, step = 128)\n",
        "    else:\n",
        "        batch_size = trial.suggest_int('training batch size', 128, 512, step = 128)\n",
        "\n",
        "    bins = trial.suggest_int('num feature levels', 20, 35)\n",
        "    embedding_dimension = trial.suggest_int('feature embedding_dimension', 150, 250)\n",
        "\n",
        "    optimizer = trial.suggest_categorical('module optimizer', [ 'adam', 'adamw'])\n",
        "    hidden_init = trial.suggest_categorical('MLP initialization', [ 'he', 'xavier'])\n",
        "    hidden_nodes_layer1 = trial.suggest_int('num of hidden nodes in 1st hidden layer', 30, 150)\n",
        "    hidden_nodes_layer2 = trial.suggest_int('num of hidden nodes in 2nd hidden layer', 30, hidden_nodes_layer1)\n",
        "    activation = trial.suggest_categorical('MLP activation function', [ 'relu', 'leakyrelu', 'elu', 'gelu', 'tanh'])\n",
        "    learning_rate = trial.suggest_float('Adam learning rate', 0.0001, 0.0003, step = 0.00005)\n",
        "    weight_decays = trial.suggest_float(\"Adam weight_decay\", 1e-6, 1e-3, log = True)\n",
        "    reg_frequency = trial.suggest_int('reg computation frequency', 50, 55)\n",
        "    le_reg_weight = trial.suggest_float( 'le reg re-weight', 0.001, 0.021, step = 0.001)\n",
        "    ge_reg_weight = trial.suggest_float( 'ge reg re-weight', 0.001, 0.021, step = 0.001)\n",
        "\n",
        "    if (len(cat_features) > 0):\n",
        "        be_reg_weight = trial.suggest_float( 'be reg re-weight', 0.00001, 0.001, step = 0.0001)\n",
        "    # --------------------------------------------------------------------\n",
        "\n",
        "    # # Or, comment out above and manually set the hyper-param for some initial results (please tune for best performance):\n",
        "    # global bayes_opt\n",
        "    # bayes_opt = False\n",
        "\n",
        "    # fake_tune = trial.suggest_categorical( 'fake tune', [ 'a', 'b'] )\n",
        "    # normalize = 0\n",
        "    # batch_size = 128\n",
        "    # bins = 21\n",
        "    # embedding_dimension = 241\n",
        "    # optimizer = 'adam'\n",
        "    # hidden_init = 'he'\n",
        "    # hidden_nodes_layer1 = 58\n",
        "    # hidden_nodes_layer2 = 47\n",
        "    # activation = 'elu'\n",
        "    # learning_rate = 0.001\n",
        "    # weight_decays = 1.41e-05\n",
        "    # reg_frequency = 33 # 11 x 3\n",
        "    # le_reg_weight = 0.005\n",
        "    # ge_reg_weight = 0.005\n",
        "    # if (len(cat_features) > 0):\n",
        "    #     be_reg_weight = 0.00005\n",
        "    # # --------------------------------------------------------------------\n",
        "\n",
        "    train_copy = train_data.copy()\n",
        "\n",
        "    if normalize == '1':\n",
        "        # perform quantile transform same as in the FT-Trans paper:\n",
        "        objective.qt = QuantileTransformer(n_quantiles = 1000, output_distribution = 'normal', subsample = int(1e9), random_state = 0)\n",
        "\n",
        "        nums = []\n",
        "        for i in train_copy.columns:\n",
        "            if i != 'target':\n",
        "                if pd.api.types.is_numeric_dtype(train_copy[i]):\n",
        "                    nums.append(i)\n",
        "\n",
        "        converted_nums = objective.qt.fit_transform( train_copy[nums] )\n",
        "        train_copy[nums] = converted_nums\n",
        "\n",
        "\n",
        "    new_train, cut_off_dict, feature_lookup_dict, cat_bins_total = FeatureTransfers(train_copy, bins)\n",
        "\n",
        "    for k,v in feature_lookup_dict.items():\n",
        "        print(k, '   ', v)\n",
        "    print()\n",
        "    print()\n",
        "\n",
        "    total_numerical_nodes = len(le_nodes) + len(ge_nodes)\n",
        "    total_levels = num_of_numerical_predictors * bins + sum(cat_bins_total)\n",
        "    print(total_levels, embedding_dimension)\n",
        "    print(be_node_counter, embedding_dimension)\n",
        "\n",
        "    # convert levels to indexes according to the lookup dict; for embedding vector retrival later\n",
        "    levels_to_index_df = new_train.copy()\n",
        "\n",
        "    for i in levels_to_index_df.columns:\n",
        "        if (i != 'target'):\n",
        "            for level in levels_to_index_df[i].unique():\n",
        "                levels_to_index_df[i] = levels_to_index_df[i].replace(level, feature_lookup_dict[level])\n",
        "\n",
        "    print(levels_to_index_df.head(5))\n",
        "    levels_to_index_df_np = levels_to_index_df.to_numpy()\n",
        "\n",
        "    train_loader = torch.utils.data.DataLoader(levels_to_index_df_np, batch_size = batch_size, shuffle = True)\n",
        "\n",
        "\n",
        "    # setup val_loader\n",
        "    val_copy = val_data.copy()\n",
        "\n",
        "    if normalize == '1':\n",
        "        converted_nums = objective.qt.transform( val_copy[nums] )\n",
        "        val_copy[nums] = converted_nums\n",
        "\n",
        "    if cat_as_num == True:\n",
        "        # for non-ordinal Cat feature, perform target encoding via encoders from training set\n",
        "        for i in val_copy.columns:\n",
        "            if (i != 'target'):\n",
        "                if pd.api.types.is_numeric_dtype(val_copy[i]) == False:\n",
        "                    encoder = encoder_dict[i]\n",
        "                    val_copy[i] = encoder.transform( val_copy[i] )\n",
        "\n",
        "    for i in val_copy.columns:\n",
        "        if ('target' in i) == False:\n",
        "            if pd.api.types.is_numeric_dtype(val_copy[i]):\n",
        "\n",
        "                cut_offs = cut_off_dict[i]\n",
        "\n",
        "                #  1st cutoff  convert  = 1st cutoff\n",
        "                #  last cutoff  convert  = last cutoff\n",
        "                val_copy.loc[val_copy[i] < cut_offs[0], i] = cut_offs[0]\n",
        "                val_copy.loc[val_copy[i] > cut_offs[len(cut_offs)-1], i] = cut_offs[len(cut_offs)-1]\n",
        "\n",
        "                # use cut-offs generated from only train set to perform conversion to avoid leakage\n",
        "                val_copy[i] = pd.cut(val_copy[i], cut_offs, duplicates = 'drop', right = True, include_lowest = True)\n",
        "                val_copy[i] = i + '_' + val_copy[i].astype(str)\n",
        "\n",
        "            else:\n",
        "                val_copy[i] = i + '_' + val_copy[i].astype(str)\n",
        "\n",
        "    # Now create data loader for val set\n",
        "    levels_to_index_df_val = val_copy.iloc[:,:-1] # get all but last (target) feature\n",
        "\n",
        "    for i in levels_to_index_df_val.columns:\n",
        "        for level in levels_to_index_df_val[i].unique():\n",
        "            levels_to_index_df_val[i] = levels_to_index_df_val[i].replace(level, feature_lookup_dict[level])\n",
        "\n",
        "    print(levels_to_index_df_val.head(5))\n",
        "    levels_to_index_df_val_np = levels_to_index_df_val.to_numpy()\n",
        "\n",
        "    val_loader = torch.utils.data.DataLoader(levels_to_index_df_val_np, batch_size = batch_size, shuffle = False)\n",
        "\n",
        "\n",
        "    # setup test_loader\n",
        "    test_copy = test_data.copy()\n",
        "\n",
        "    if normalize == '1':\n",
        "        converted_nums = objective.qt.transform( test_copy[nums] )\n",
        "        test_copy[nums] = converted_nums\n",
        "\n",
        "    if cat_as_num == True:\n",
        "        # for non-ordinal Cat feature, perform target encoding via encoders from training set\n",
        "        for i in test_copy.columns:\n",
        "            if (i != 'target'):\n",
        "                if pd.api.types.is_numeric_dtype(test_copy[i]) == False:\n",
        "                    encoder = encoder_dict[i]\n",
        "                    test_copy[i] = encoder.transform( test_copy[i] )\n",
        "\n",
        "    for i in test_copy.columns:\n",
        "        if ('target' in i) == False:\n",
        "            if pd.api.types.is_numeric_dtype(test_copy[i]):\n",
        "\n",
        "                cut_offs = cut_off_dict[i]\n",
        "\n",
        "                #  1st cutoff  convert  = 1st cutoff\n",
        "                #  last cutoff  convert  = last cutoff\n",
        "                test_copy.loc[test_copy[i] < cut_offs[0], i] = cut_offs[0]\n",
        "                test_copy.loc[test_copy[i] > cut_offs[len(cut_offs)-1], i] = cut_offs[len(cut_offs)-1]\n",
        "\n",
        "                # use cut-offs generated from only train set to perform conversion to avoid leakage\n",
        "                test_copy[i] = pd.cut(test_copy[i], cut_offs, duplicates = 'drop', right = True, include_lowest = True)\n",
        "                test_copy[i] = i + '_' + test_copy[i].astype(str)\n",
        "\n",
        "            else:\n",
        "                test_copy[i] = i + '_' + test_copy[i].astype(str)\n",
        "\n",
        "    # Now create data loader for test set\n",
        "    levels_to_index_df_test = test_copy.iloc[:,:-1] # get all but last (target) feature\n",
        "\n",
        "    for i in levels_to_index_df_test.columns:\n",
        "        for level in levels_to_index_df_test[i].unique():\n",
        "            levels_to_index_df_test[i] = levels_to_index_df_test[i].replace(level, feature_lookup_dict[level])\n",
        "\n",
        "    print(levels_to_index_df_test.head(5))\n",
        "    levels_to_index_df_test_np = levels_to_index_df_test.to_numpy()\n",
        "\n",
        "    test_loader = torch.utils.data.DataLoader(levels_to_index_df_test_np, batch_size = batch_size, shuffle = False)\n",
        "\n",
        "\n",
        "    # get clf & evaluate performance\n",
        "    if len(cat_features) > 0:\n",
        "        clf, val_accuracy, test_accuracy, init_num_regs_losses, end_num_regs_losses, init_cat_regs_losses, end_cat_regs_losses = Train_NSDT_split_regs(masking, train_loader, val_loader, test_loader, early_stopper, new_train, cut_off_dict, feature_lookup_dict, cat_bins_total, total_numerical_nodes, total_levels, bins,\n",
        "                                    embedding_dimension, epochs, optimizer, hidden_init, hidden_nodes_layer1, hidden_nodes_layer2, activation, learning_rate, weight_decays, reg_frequency, le_reg_weight, ge_reg_weight, be_reg_weight)\n",
        "    else:\n",
        "        clf, val_accuracy, test_accuracy, init_num_regs_losses, end_num_regs_losses = Train_num_feature_only_NSDT_split_regs(masking, train_loader, val_loader, test_loader, early_stopper, new_train, cut_off_dict, feature_lookup_dict, cat_bins_total, total_numerical_nodes, total_levels, bins,\n",
        "                                    embedding_dimension, epochs, optimizer, hidden_init, hidden_nodes_layer1, hidden_nodes_layer2, activation, learning_rate, weight_decays, reg_frequency, le_reg_weight, ge_reg_weight)\n",
        "\n",
        "    if len(cat_features) > 0:\n",
        "        if end_num_regs_losses >= init_num_regs_losses:\n",
        "            reg_accuracy = 0.5 * 0.0\n",
        "        else:\n",
        "            reg_accuracy = 0.5 * float( (init_num_regs_losses - end_num_regs_losses) / init_num_regs_losses )\n",
        "\n",
        "        if end_cat_regs_losses >= init_cat_regs_losses:\n",
        "            reg_accuracy += 0.5 * 0.0\n",
        "        else:\n",
        "            reg_accuracy += 0.5 * float( (init_cat_regs_losses - end_cat_regs_losses) / init_cat_regs_losses )\n",
        "\n",
        "    else:\n",
        "        if end_num_regs_losses >= init_num_regs_losses:\n",
        "            reg_accuracy = 0.0\n",
        "        else:\n",
        "            reg_accuracy = float( (init_num_regs_losses - end_num_regs_losses) / init_num_regs_losses )\n",
        "\n",
        "    # In case of internet break or memory loss; display test accuracy for each search round\n",
        "    # Note: please report the test accuracy for trail with highest performance on val set\n",
        "    print('----------------------- For Trial ', trial.number, ': ', 'test accuracy:', test_accuracy, ' reg loss decreased by (%):', reg_accuracy)\n",
        "    print()\n",
        "    print()\n",
        "\n",
        "    # save clf to a file for future use\n",
        "    with open( ('{}' + '_' + dataset + '_' + model_type + '.pickle').format(trial.number), 'wb' ) as fout:\n",
        "        pickle.dump(clf, fout)\n",
        "\n",
        "    return val_accuracy # use val accuracy to search for best hyper-param set\n",
        "\n",
        "\n",
        "# start hyper-param tuning\n",
        "study = optuna.create_study(direction = 'maximize')\n",
        "study.optimize(objective, n_trials = 36) # set n_trials = 1 if not tuning (we have set a dummy variable named 'fake_tune' to enable model training)\n",
        "\n",
        "with open( ('{}' + '_' + dataset + '_' + model_type + '.pickle').format(study.best_trial.number), 'rb' ) as fin:\n",
        "    clf = pickle.load(fin)\n",
        "\n",
        "# In case you did not perform bayes opt while using the pre-defined hyper-param values;\n",
        "if bayes_opt == False:\n",
        "    batch_size = 128\n",
        "    bins = 21\n",
        "\n",
        "if bayes_opt:\n",
        "    batch_size = study.best_params['training batch size']\n",
        "    bins = study.best_params['num feature levels']\n",
        "\n",
        "    print()\n",
        "    for key, value in study.best_params.items():\n",
        "        print( \"    {}: {}\".format(key, value) )\n",
        "\n",
        "    print()\n",
        "    print()\n",
        "    importance_dict = optuna.importance.get_param_importances(study)\n",
        "    for key, value in importance_dict.items():\n",
        "        print( \"    {}: {}\".format(key, value) )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "AHcOtBV7vKip"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rdq9eTqLPe35"
      },
      "outputs": [],
      "source": [
        "if bayes_opt:\n",
        "    if study.best_params['normalize num features'] == '1':\n",
        "\n",
        "        # perform quantile transform as in the FT-Trans paper:\n",
        "        nums = []\n",
        "        for i in train_data.columns:\n",
        "            if i != 'target':\n",
        "                if pd.api.types.is_numeric_dtype(train_data[i]):\n",
        "                    nums.append(i)\n",
        "\n",
        "        converted_nums = objective.qt.transform( train_data[nums] )\n",
        "        train_data[nums] = converted_nums\n",
        "\n",
        "        converted_nums = objective.qt.transform( val_data[nums] )\n",
        "        val_data[nums] = converted_nums\n",
        "\n",
        "\n",
        "new_train, cut_off_dict, feature_lookup_dict, cat_bins_total = FeatureTransfers( train_data, bins )\n",
        "\n",
        "for k,v in feature_lookup_dict.items():\n",
        "    print(k, '   ', v)\n",
        "print()\n",
        "print()\n",
        "\n",
        "total_numerical_nodes = len(le_nodes) + len(ge_nodes)\n",
        "total_levels = num_of_numerical_predictors * bins + sum(cat_bins_total)\n",
        "\n",
        "# double-check to verify if matching with above\n",
        "print(total_levels, bins)\n",
        "print(be_node_counter, bins)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y00zEY9Tqljr"
      },
      "source": [
        "# Model Testing:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q08hi6DEqqjg"
      },
      "outputs": [],
      "source": [
        "if bayes_opt:\n",
        "    if study.best_params['normalize num features'] == '1':\n",
        "\n",
        "        converted_nums = objective.qt.transform( test_data[nums] )\n",
        "        test_data[nums] = converted_nums\n",
        "\n",
        "\n",
        "if cat_as_num == True:\n",
        "    # for non-ordinal Cat feature, perform target encoding via encoders from training set\n",
        "    for i in test_data.columns:\n",
        "        if (i != 'target'):\n",
        "            if pd.api.types.is_numeric_dtype(test_data[i]) == False:\n",
        "                encoder = encoder_dict[i]\n",
        "                test_data[i] = encoder.transform( test_data[i] )\n",
        "\n",
        "\n",
        "for i in test_data.columns:\n",
        "    if ('target' in i) == False:\n",
        "        if pd.api.types.is_numeric_dtype(test_data[i]):\n",
        "\n",
        "            cut_offs = cut_off_dict[i]\n",
        "\n",
        "            #  1st cutoff  convert  = 1st cutoff\n",
        "            #  last cutoff  convert  = last cutoff\n",
        "            test_data.loc[test_data[i] < cut_offs[0], i] = cut_offs[0]\n",
        "            test_data.loc[test_data[i] > cut_offs[len(cut_offs)-1], i] = cut_offs[len(cut_offs)-1]\n",
        "\n",
        "            # use cut-offs generated from only train set to perform conversion to avoid leakage\n",
        "            test_data[i] = pd.cut(test_data[i], cut_offs, duplicates = 'drop', right = True, include_lowest = True)\n",
        "            test_data[i] = i + '_' + test_data[i].astype(str)\n",
        "\n",
        "        else:\n",
        "            test_data[i] = i + '_' + test_data[i].astype(str)\n",
        "\n",
        "\n",
        "# Now create  data loader for testing set\n",
        "test_copy = test_data.copy()\n",
        "levels_to_index_df_test = test_copy.iloc[:,:-1] # get all but last (target) feature\n",
        "\n",
        "for i in levels_to_index_df_test.columns:\n",
        "    for level in levels_to_index_df_test[i].unique():\n",
        "        levels_to_index_df_test[i] = levels_to_index_df_test[i].replace(level, feature_lookup_dict[level])\n",
        "\n",
        "print(levels_to_index_df_test.head(5))\n",
        "levels_to_index_df_test_np = levels_to_index_df_test.to_numpy()\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(levels_to_index_df_test_np, batch_size = batch_size, shuffle = False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "XWQTnpwdqsdZ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "lCea_foOfzYA"
      },
      "outputs": [],
      "source": [
        "# Create context value index lists for training set:\n",
        "all_batch_le_context_value_index_list = []\n",
        "all_batch_ge_context_value_index_list = []\n",
        "all_batch_be_context_value_index_list = []\n",
        "\n",
        "for batch in test_loader:\n",
        "\n",
        "    batch_size = len(batch)\n",
        "    batch_le_context_value_index_list = []\n",
        "    batch_ge_context_value_index_list = []\n",
        "    batch_be_context_value_index_list = []\n",
        "\n",
        "    for node in le_nodes:\n",
        "        le_context_value_index_list = [ node[2] ] * batch_size\n",
        "        batch_le_context_value_index_list.append(le_context_value_index_list)\n",
        "\n",
        "    for node in ge_nodes:\n",
        "        ge_context_value_index_list = [ node[2] ] * batch_size\n",
        "        batch_ge_context_value_index_list.append(ge_context_value_index_list)\n",
        "\n",
        "    for node in be_nodes:\n",
        "        be_context_value_index_list = [ node[2] ] * batch_size\n",
        "        batch_be_context_value_index_list.append(be_context_value_index_list)\n",
        "\n",
        "    all_batch_le_context_value_index_list.append(batch_le_context_value_index_list)\n",
        "    all_batch_ge_context_value_index_list.append(batch_ge_context_value_index_list)\n",
        "    all_batch_be_context_value_index_list.append(batch_be_context_value_index_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "qsxRYdcvfzds"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k_wzdRp79q7-"
      },
      "outputs": [],
      "source": [
        "# first convert test set feature levels to indexes just like training set\n",
        "# use predictor features (X) only to do performance eval.\n",
        "\n",
        "# start testing process:\n",
        "feature_indexes_le_nodes  = []\n",
        "feature_indexes_ge_nodes  = []\n",
        "feature_indexes_be_nodes  = []\n",
        "\n",
        "for node in le_nodes:\n",
        "    feature_indexes_le_nodes.append(node[0] - 1)\n",
        "\n",
        "for node in ge_nodes:\n",
        "    feature_indexes_ge_nodes.append(node[0] - 1)\n",
        "\n",
        "for node in be_nodes:\n",
        "    feature_indexes_be_nodes.append(node[0] - 1)\n",
        "\n",
        "clf = clf.eval()\n",
        "\n",
        "pred_labels = []\n",
        "targets = []\n",
        "\n",
        "batch_count = 0\n",
        "for batch in test_loader:\n",
        "\n",
        "    subset = batch[:, :] # X only; target already removed\n",
        "    batch_size = len(batch)\n",
        "\n",
        "    if (len(cat_features) > 0):\n",
        "        batch_le_feature_index_list = subset[:, feature_indexes_le_nodes].permute(1,0).to('cuda')\n",
        "        batch_ge_feature_index_list = subset[:, feature_indexes_ge_nodes].permute(1,0).to('cuda')\n",
        "        batch_be_feature_index_list = subset[:, feature_indexes_be_nodes].permute(1,0).to('cuda')\n",
        "\n",
        "        batch_le_context_value_index_list = all_batch_le_context_value_index_list[batch_count]\n",
        "        batch_ge_context_value_index_list = all_batch_ge_context_value_index_list[batch_count]\n",
        "        batch_be_context_value_index_list = all_batch_be_context_value_index_list[batch_count]\n",
        "\n",
        "        preds = clf.forward(batch_le_feature_index_list, batch_le_context_value_index_list,\n",
        "                            batch_ge_feature_index_list, batch_ge_context_value_index_list,\n",
        "                            batch_be_feature_index_list, batch_be_context_value_index_list,\n",
        "                            batch_size )\n",
        "    else:\n",
        "        batch_le_feature_index_list = subset[:, feature_indexes_le_nodes].permute(1, 0).to('cuda')\n",
        "        batch_ge_feature_index_list = subset[:, feature_indexes_ge_nodes].permute(1, 0).to('cuda')\n",
        "\n",
        "        batch_le_context_value_index_list = all_batch_le_context_value_index_list[batch_count]\n",
        "        batch_ge_context_value_index_list = all_batch_ge_context_value_index_list[batch_count]\n",
        "\n",
        "        preds = clf.forward(batch_le_feature_index_list, batch_le_context_value_index_list,\n",
        "                            batch_ge_feature_index_list, batch_ge_context_value_index_list,\n",
        "                            batch_size )\n",
        "\n",
        "    preds = torch.sigmoid(preds)\n",
        "    preds = torch.squeeze(preds).tolist()\n",
        "\n",
        "    batch_count += 1\n",
        "\n",
        "    for i in preds:\n",
        "        if i > 0.5:\n",
        "            pred_labels.append(1)\n",
        "        if i <= 0.5:\n",
        "            pred_labels.append(0)\n",
        "\n",
        "# Compute Balanced Accuracy:\n",
        "accuracy = metrics.balanced_accuracy_score(test_data['target'], pred_labels)\n",
        "print('Balanced Accuracy: ', accuracy)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}